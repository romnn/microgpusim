GPGPU-Sim version 4.2.0 (build gpgpu-sim-modified) configured with AccelWattch.
setup_environment succeeded
Accel-Sim [build <box>]
GPGPU-Sim: Registered options:

-save_embedded_ptx                      0 # saves ptx files embedded in binary as <n>.ptx
-keep                                   0 # keep intermediate files created by GPGPU-Sim when interfacing with external programs
-gpgpu_ptx_save_converted_ptxplus                    0 # Saved converted ptxplus to a file
-gpgpu_occupancy_sm_number                    0 # The SM number to pass to ptxas when getting register usage for computing GPU occupancy. This parameter is required in the config.
-ptx_opcode_latency_int     1,1,19,25,145,32 # Opcode latencies for integers <ADD,MAX,MUL,MAD,DIV,SHFL>Default 1,1,19,25,145,32
-ptx_opcode_latency_fp           1,1,1,1,30 # Opcode latencies for single precision floating points <ADD,MAX,MUL,MAD,DIV>Default 1,1,1,1,30
-ptx_opcode_latency_dp          8,8,8,8,335 # Opcode latencies for double precision floating points <ADD,MAX,MUL,MAD,DIV>Default 8,8,8,8,335
-ptx_opcode_latency_sfu                    8 # Opcode latencies for SFU instructionsDefault 8
-ptx_opcode_latency_tesnor                   64 # Opcode latencies for Tensor instructionsDefault 64
-ptx_opcode_initiation_int         1,1,4,4,32,4 # Opcode initiation intervals for integers <ADD,MAX,MUL,MAD,DIV,SHFL>Default 1,1,4,4,32,4
-ptx_opcode_initiation_fp            1,1,1,1,5 # Opcode initiation intervals for single precision floating points <ADD,MAX,MUL,MAD,DIV>Default 1,1,1,1,5
-ptx_opcode_initiation_dp          8,8,8,8,130 # Opcode initiation intervals for double precision floating points <ADD,MAX,MUL,MAD,DIV>Default 8,8,8,8,130
-ptx_opcode_initiation_sfu                    8 # Opcode initiation intervals for sfu instructionsDefault 8
-ptx_opcode_initiation_tensor                   64 # Opcode initiation intervals for tensor instructionsDefault 64
-cdp_latency         7200,8000,100,12000,1600 # CDP API latency <cudaStreamCreateWithFlags, cudaGetParameterBufferV2_init_perWarp, cudaGetParameterBufferV2_perKernel, cudaLaunchDeviceV2_init_perWarp, cudaLaunchDevicV2_perKernel>Default 7200,8000,100,12000,1600
-network_mode                           1 # Interconnection network mode
-inter_config_file                   mesh # Interconnection network config file
-icnt_in_buffer_limit                   64 # in_buffer_limit
-icnt_out_buffer_limit                   64 # out_buffer_limit
-icnt_subnets                           2 # subnets
-icnt_arbiter_algo                      1 # arbiter_algo
-icnt_verbose                           0 # inct_verbose
-icnt_grant_cycles                      1 # grant_cycles
-gpgpu_ptx_use_cuobjdump                    0 # Use cuobjdump to extract ptx and sass from binaries
-gpgpu_experimental_lib_support                    0 # Try to extract code from cuda libraries [Broken because of unknown cudaGetExportTable]
-checkpoint_option                      0 #  checkpointing flag (0 = no checkpoint)
-checkpoint_kernel                      1 #  checkpointing during execution of which kernel (1- 1st kernel)
-checkpoint_CTA                         0 #  checkpointing after # of CTA (< less than total CTA)
-resume_option                          0 #  resume flag (0 = no resume)
-resume_kernel                          0 #  Resume from which kernel (1= 1st kernel)
-resume_CTA                             0 #  resume from which CTA 
-checkpoint_CTA_t                       0 #  resume from which CTA 
-checkpoint_insn_Y                      0 #  resume from which CTA 
-gpgpu_ptx_convert_to_ptxplus                    0 # Convert SASS (native ISA) to ptxplus and run ptxplus
-gpgpu_ptx_force_max_capability                    0 # Force maximum compute capability
-gpgpu_ptx_inst_debug_to_file                    0 # Dump executed instructions' debug information to file
-gpgpu_ptx_inst_debug_file       inst_debug.txt # Executed instructions' debug output file
-gpgpu_ptx_inst_debug_thread_uid                    1 # Thread UID for executed instructions' debug output
-gpgpu_simd_model                       1 # 1 = post-dominator
-gpgpu_shader_core_pipeline              1024:32 # shader core pipeline config, i.e., {<nthread>:<warpsize>}
-gpgpu_tex_cache:l1  8:128:5,L:R:m:N,F:128:4,128:2 # per-shader L1 texture cache  (READ-ONLY) config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>:<rf>}
-gpgpu_const_cache:l1 64:64:2,L:R:f:N,A:2:32,4 # per-shader L1 constant memory cache  (READ-ONLY) config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>} 
-gpgpu_cache:il1     4:256:4,L:R:f:N,A:2:32,4 # shader L1 instruction cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>} 
-gpgpu_cache:dl1                     none # per-shader L1 data cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}
-gpgpu_l1_cache_write_ratio                    0 # L1D write ratio
-gpgpu_l1_banks                         1 # The number of L1 cache banks
-gpgpu_l1_banks_byte_interleaving                   32 # l1 banks byte interleaving granularity
-gpgpu_l1_banks_hashing_function                    0 # l1 banks hashing function
-gpgpu_l1_latency                       1 # L1 Hit Latency
-gpgpu_smem_latency                     3 # smem Latency
-gpgpu_cache:dl1PrefL1                 none # per-shader L1 data cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}
-gpgpu_cache:dl1PrefShared                 none # per-shader L1 data cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}
-gpgpu_gmem_skip_L1D                    0 # global memory access skip L1D cache (implements -Xptxas -dlcm=cg, default=no skip)
-gpgpu_perfect_mem                      0 # enable perfect memory mode (no cache miss)
-n_regfile_gating_group                    4 # group of lanes that should be read/written together)
-gpgpu_clock_gated_reg_file                    0 # enable clock gated reg file for power calculations
-gpgpu_clock_gated_lanes                    0 # enable clock gated lanes for power calculations
-gpgpu_shader_registers                 8192 # Number of registers per shader core. Limits number of concurrent CTAs. (default 8192)
-gpgpu_registers_per_block                 8192 # Maximum number of registers per CTA. (default 8192)
-gpgpu_ignore_resources_limitation                    0 # gpgpu_ignore_resources_limitation (default 0)
-gpgpu_shader_cta                      32 # Maximum number of concurrent CTAs in shader (default 32)
-gpgpu_num_cta_barriers                   16 # Maximum number of named barriers per CTA (default 16)
-gpgpu_n_clusters                      10 # number of processing clusters
-gpgpu_n_cores_per_cluster                    3 # number of simd cores per cluster
-gpgpu_n_cluster_ejection_buffer_size                    8 # number of packets in ejection buffer
-gpgpu_n_ldst_response_buffer_size                    2 # number of response packets in ld/st unit ejection buffer
-gpgpu_shmem_per_block                49152 # Size of shared memory per thread block or CTA (default 48kB)
-gpgpu_shmem_size                   16384 # Size of shared memory per shader core (default 16kB)
-gpgpu_shmem_option                     0 # Option list of shared memory sizes
-gpgpu_unified_l1d_size                    0 # Size of unified data cache(L1D + shared memory) in KB
-gpgpu_adaptive_cache_config                    0 # adaptive_cache_config
-gpgpu_shmem_sizeDefault                16384 # Size of shared memory per shader core (default 16kB)
-gpgpu_shmem_size_PrefL1                16384 # Size of shared memory per shader core (default 16kB)
-gpgpu_shmem_size_PrefShared                16384 # Size of shared memory per shader core (default 16kB)
-gpgpu_shmem_num_banks                   16 # Number of banks in the shared memory in each shader core (default 16)
-gpgpu_shmem_limited_broadcast                    1 # Limit shared memory to do one broadcast per cycle (default on)
-gpgpu_shmem_warp_parts                    2 # Number of portions a warp is divided into for shared memory bank conflict check 
-gpgpu_mem_unit_ports                    1 # The number of memory transactions allowed per core cycle
-gpgpu_shmem_warp_parts                    2 # Number of portions a warp is divided into for shared memory bank conflict check 
-gpgpu_warpdistro_shader                   -1 # Specify which shader core to collect the warp size distribution from
-gpgpu_warp_issue_shader                    0 # Specify which shader core to collect the warp issue distribution from
-gpgpu_local_mem_map                    1 # Mapping from local memory space address to simulated GPU physical address space (default = enabled)
-gpgpu_num_reg_banks                    8 # Number of register banks (default = 8)
-gpgpu_reg_bank_use_warp_id                    0 # Use warp ID in mapping registers to banks (default = off)
-gpgpu_sub_core_model                    0 # Sub Core Volta/Pascal model (default = off)
-gpgpu_enable_specialized_operand_collector                    1 # enable_specialized_operand_collector
-gpgpu_operand_collector_num_units_sp                    4 # number of collector units (default = 4)
-gpgpu_operand_collector_num_units_dp                    0 # number of collector units (default = 0)
-gpgpu_operand_collector_num_units_sfu                    4 # number of collector units (default = 4)
-gpgpu_operand_collector_num_units_int                    0 # number of collector units (default = 0)
-gpgpu_operand_collector_num_units_tensor_core                    4 # number of collector units (default = 4)
-gpgpu_operand_collector_num_units_mem                    2 # number of collector units (default = 2)
-gpgpu_operand_collector_num_units_gen                    0 # number of collector units (default = 0)
-gpgpu_operand_collector_num_in_ports_sp                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_dp                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_in_ports_sfu                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_int                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_in_ports_tensor_core                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_mem                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_gen                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_out_ports_sp                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_dp                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_out_ports_sfu                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_int                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_out_ports_tensor_core                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_mem                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_gen                    0 # number of collector unit in ports (default = 0)
-gpgpu_coalesce_arch                   13 # Coalescing arch (GT200 = 13, Fermi = 20)
-gpgpu_num_sched_per_core                    1 # Number of warp schedulers per core
-gpgpu_max_insn_issue_per_warp                    2 # Max number of instructions that can be issued per warp in one cycle by scheduler (either 1 or 2)
-gpgpu_dual_issue_diff_exec_units                    1 # should dual issue use two different execution unit resources (Default = 1)
-gpgpu_simt_core_sim_order                    1 # Select the simulation order of cores in a cluster (0=Fix, 1=Round-Robin)
-gpgpu_pipeline_widths 1,1,1,1,1,1,1,1,1,1,1,1,1 # Pipeline widths ID_OC_SP,ID_OC_DP,ID_OC_INT,ID_OC_SFU,ID_OC_MEM,OC_EX_SP,OC_EX_DP,OC_EX_INT,OC_EX_SFU,OC_EX_MEM,EX_WB,ID_OC_TENSOR_CORE,OC_EX_TENSOR_CORE
-gpgpu_tensor_core_avail                    0 # Tensor Core Available (default=0)
-gpgpu_num_sp_units                     1 # Number of SP units (default=1)
-gpgpu_num_dp_units                     0 # Number of DP units (default=0)
-gpgpu_num_int_units                    0 # Number of INT units (default=0)
-gpgpu_num_sfu_units                    1 # Number of SF units (default=1)
-gpgpu_num_tensor_core_units                    0 # Number of tensor_core units (default=1)
-gpgpu_num_mem_units                    1 # Number if ldst units (default=1) WARNING: not hooked up to anything
-gpgpu_scheduler                      gto # Scheduler configuration: < lrr | gto | two_level_active > If two_level_active:<num_active_warps>:<inner_prioritization>:<outer_prioritization>For complete list of prioritization values see shader.h enum scheduler_prioritization_typeDefault: gto
-gpgpu_concurrent_kernel_sm                    0 # Support concurrent kernels on a SM (default = disabled)
-gpgpu_perfect_inst_const_cache                    0 # perfect inst and const cache mode, so all inst and const hits in the cache(default = disabled)
-gpgpu_inst_fetch_throughput                    1 # the number of fetched intruction per warp each cycle
-gpgpu_reg_file_port_throughput                    1 # the number ports of the register file
-specialized_unit_1         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_2         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_3         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_4         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_5         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_6         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_7         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_8         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-gpgpu_perf_sim_memcpy                    1 # Fill the L2 cache on memcpy
-gpgpu_simple_dram_model                    0 # simple_dram_model with fixed latency and BW
-gpgpu_dram_scheduler                    1 # 0 = fifo, 1 = FR-FCFS (defaul)
-gpgpu_dram_partition_queues              8:8:8:8 # i2$:$2d:d2$:$2i
-l2_ideal                               0 # Use a ideal L2 cache that always hit
-gpgpu_cache:dl2     64:128:8,L:B:m:N,A:16:4,4 # unified banked L2 data cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>}
-gpgpu_cache:dl2_texture_only                    1 # L2 cache used for texture only
-gpgpu_n_mem                            8 # number of memory modules (e.g. memory controllers) in gpu
-gpgpu_n_sub_partition_per_mchannel                    1 # number of memory subpartition in each memory module
-gpgpu_n_mem_per_ctrlr                    1 # number of memory chips per memory controller
-gpgpu_memlatency_stat                    0 # track and display latency statistics 0x2 enables MC, 0x4 enables queue logs
-gpgpu_frfcfs_dram_sched_queue_size                    0 # 0 = unlimited (default); # entries per chip
-gpgpu_dram_return_queue_size                    0 # 0 = unlimited (default); # entries per chip
-gpgpu_dram_buswidth                    4 # default = 4 bytes (8 bytes per cycle at DDR)
-gpgpu_dram_burst_length                    4 # Burst length of each DRAM request (default = 4 data bus cycle)
-dram_data_command_freq_ratio                    2 # Frequency ratio between DRAM data bus and command bus (default = 2 times, i.e. DDR)
-gpgpu_dram_timing_opt 4:2:8:12:21:13:34:9:4:5:13:1:0:0 # DRAM timing parameters = {nbk:tCCD:tRRD:tRCD:tRAS:tRP:tRC:CL:WL:tCDLR:tWR:nbkgrp:tCCDL:tRTPL}
-gpgpu_l2_rop_latency                   85 # ROP queue latency (default 85)
-dram_latency                          30 # DRAM latency (default 30)
-dram_dual_bus_interface                    0 # dual_bus_interface (default = 0) 
-dram_bnk_indexing_policy                    0 # dram_bnk_indexing_policy (0 = normal indexing, 1 = Xoring with the higher bits) (Default = 0)
-dram_bnkgrp_indexing_policy                    0 # dram_bnkgrp_indexing_policy (0 = take higher bits, 1 = take lower bits) (Default = 0)
-dram_seperate_write_queue_enable                    0 # Seperate_Write_Queue_Enable
-dram_write_queue_size             32:28:16 # Write_Queue_Size
-dram_elimnate_rw_turnaround                    0 # elimnate_rw_turnaround i.e set tWTR and tRTW = 0
-icnt_flit_size                        32 # icnt_flit_size
-gpgpu_mem_addr_mapping                 NULL # mapping memory address to dram model {dramid@<start bit>;<memory address map>}
-gpgpu_mem_addr_test                    0 # run sweep test to check address mapping for aliased address
-gpgpu_mem_address_mask                    0 # 0 = old addressing mask, 1 = new addressing mask, 2 = new add. mask + flipped bank sel and chip sel bits
-gpgpu_memory_partition_indexing                    0 # 0 = no indexing, 1 = bitwise xoring, 2 = IPoly, 3 = custom indexing
-accelwattch_xml_file accelwattch_sass_sim.xml # AccelWattch XML file
-power_simulation_enabled                    0 # Turn on power simulator (1=On, 0=Off)
-power_per_cycle_dump                    0 # Dump detailed power output each cycle
-hw_perf_file_name            hw_perf.csv # Hardware Performance Statistics file
-hw_perf_bench_name                       # Kernel Name in Hardware Performance Statistics file
-power_simulation_mode                    0 # Switch performance counter input for power simulation (0=Sim, 1=HW, 2=HW-Sim Hybrid)
-dvfs_enabled                           0 # Turn on DVFS for power model
-aggregate_power_stats                    0 # Accumulate power across all kernels
-accelwattch_hybrid_perfsim_L1_RH                    0 # Get L1 Read Hits for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L1_RM                    0 # Get L1 Read Misses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L1_WH                    0 # Get L1 Write Hits for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L1_WM                    0 # Get L1 Write Misses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L2_RH                    0 # Get L2 Read Hits for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L2_RM                    0 # Get L2 Read Misses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L2_WH                    0 # Get L2 Write Hits for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L2_WM                    0 # Get L2 Write Misses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_CC_ACC                    0 # Get Constant Cache Acesses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_SHARED_ACC                    0 # Get Shared Memory Acesses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_DRAM_RD                    0 # Get DRAM Reads for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_DRAM_WR                    0 # Get DRAM Writes for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_NOC                    0 # Get Interconnect Acesses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_PIPE_DUTY                    0 # Get Pipeline Duty Cycle Acesses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_NUM_SM_IDLE                    0 # Get Number of Idle SMs for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_CYCLES                    0 # Get Executed Cycles for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_VOLTAGE                    0 # Get Chip Voltage for Accelwattch-Hybrid from Accel-Sim
-power_trace_enabled                    0 # produce a file for the power trace (1=On, 0=Off)
-power_trace_zlevel                     6 # Compression level of the power trace output log (0=no comp, 9=highest)
-steady_power_levels_enabled                    0 # produce a file for the steady power levels (1=On, 0=Off)
-steady_state_definition                  8:4 # allowed deviation:number of samples
-gpgpu_max_cycle                        0 # terminates gpu simulation early (0 = no limit)
-gpgpu_max_insn                         0 # terminates gpu simulation early (0 = no limit)
-gpgpu_max_cta                          0 # terminates gpu simulation early (0 = no limit)
-gpgpu_max_completed_cta                    0 # terminates gpu simulation early (0 = no limit)
-gpgpu_runtime_stat               10000:0 # display runtime statistics such as dram utilization {<freq>:<flag>}
-liveness_message_freq                    1 # Minimum number of seconds between simulation liveness messages (0 = always print)
-gpgpu_compute_capability_major                    7 # Major compute capability version number
-gpgpu_compute_capability_minor                    0 # Minor compute capability version number
-gpgpu_flush_l1_cache                    0 # Flush L1 cache at the end of each kernel call
-gpgpu_flush_l2_cache                    0 # Flush L2 cache at the end of each kernel call
-gpgpu_deadlock_detect                    1 # Stop the simulation at deadlock (1=on (default), 0=off)
-gpgpu_ptx_instruction_classification                    0 # if enabled will classify ptx instruction types per kernel (Max 255 kernels now)
-gpgpu_ptx_sim_mode                     0 # Select between Performance (default) or Functional simulation (1)
-gpgpu_clock_domains 500.0:2000.0:2000.0:2000.0 # Clock Domain Frequencies in MhZ {<Core Clock>:<ICNT Clock>:<L2 Clock>:<DRAM Clock>}
-gpgpu_max_concurrent_kernel                   32 # maximum kernels that can run concurrently on GPU, set this value according to max resident grids for your compute capability
-gpgpu_cflog_interval                    0 # Interval between each snapshot in control flow logger
-visualizer_enabled                     1 # Turn on visualizer output (1=On, 0=Off)
-visualizer_outputfile                 NULL # Specifies the output log file for visualizer
-visualizer_zlevel                      6 # Compression level of the visualizer output log (0=no comp, 9=highest)
-gpgpu_stack_size_limit                 1024 # GPU thread stack size
-gpgpu_heap_size_limit              8388608 # GPU malloc heap size 
-gpgpu_runtime_sync_depth_limit                    2 # GPU device runtime synchronize depth
-gpgpu_runtime_pending_launch_count_limit                 2048 # GPU device runtime pending launch count
-trace_enabled                          0 # Turn on traces
-trace_components                    none # comma seperated list of traces to enable. Complete list found in trace_streams.tup. Default none
-trace_sampling_core                    0 # The core which is printed using CORE_DPRINTF. Default 0
-trace_sampling_memory_partition                   -1 # The memory partition which is printed using MEMPART_DPRINTF. Default -1 (i.e. all)
-enable_ptx_file_line_stats                    1 # Turn on PTX source line statistic profiling. (1 = On)
-ptx_line_stats_filename gpgpu_inst_stats.txt # Output file for PTX source line statistics.
-gpgpu_kernel_launch_latency                    0 # Kernel launch latency in cycles. Default: 0
-gpgpu_cdp_enabled                      0 # Turn on CDP
-gpgpu_TB_launch_latency                    0 # thread block launch latency in cycles. Default: 0
-trace               ./traces/kernelslist.g # traces kernel filetraces kernel file directory
-trace_opcode_latency_initiation_int                  4,1 # Opcode latencies and initiation for integers in trace driven mode <latency,initiation>
-trace_opcode_latency_initiation_sp                  4,1 # Opcode latencies and initiation for sp in trace driven mode <latency,initiation>
-trace_opcode_latency_initiation_dp                  4,1 # Opcode latencies and initiation for dp in trace driven mode <latency,initiation>
-trace_opcode_latency_initiation_sfu                  4,1 # Opcode latencies and initiation for sfu in trace driven mode <latency,initiation>
-trace_opcode_latency_initiation_tensor                  4,1 # Opcode latencies and initiation for tensor in trace driven mode <latency,initiation>
-trace_opcode_latency_initiation_spec_op_1                  4,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_2                  4,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_3                  4,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_4                  4,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_5                  4,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_6                  4,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_7                  4,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_8                  4,4 # specialized unit config <latency,initiation>
GPGPU-Sim: Configuration options:

-save_embedded_ptx                      0 # saves ptx files embedded in binary as <n>.ptx
-keep                                   0 # keep intermediate files created by GPGPU-Sim when interfacing with external programs
-gpgpu_ptx_save_converted_ptxplus                    0 # Saved converted ptxplus to a file
-gpgpu_occupancy_sm_number                   60 # The SM number to pass to ptxas when getting register usage for computing GPU occupancy. This parameter is required in the config.
-ptx_opcode_latency_int         4,13,4,5,145 # Opcode latencies for integers <ADD,MAX,MUL,MAD,DIV,SHFL>Default 1,1,19,25,145,32
-ptx_opcode_latency_fp          4,13,4,5,39 # Opcode latencies for single precision floating points <ADD,MAX,MUL,MAD,DIV>Default 1,1,1,1,30
-ptx_opcode_latency_dp         8,19,8,8,330 # Opcode latencies for double precision floating points <ADD,MAX,MUL,MAD,DIV>Default 8,8,8,8,335
-ptx_opcode_latency_sfu                    8 # Opcode latencies for SFU instructionsDefault 8
-ptx_opcode_latency_tesnor                   64 # Opcode latencies for Tensor instructionsDefault 64
-ptx_opcode_initiation_int            1,2,2,2,8 # Opcode initiation intervals for integers <ADD,MAX,MUL,MAD,DIV,SHFL>Default 1,1,4,4,32,4
-ptx_opcode_initiation_fp            1,2,1,1,4 # Opcode initiation intervals for single precision floating points <ADD,MAX,MUL,MAD,DIV>Default 1,1,1,1,5
-ptx_opcode_initiation_dp          1,2,1,1,130 # Opcode initiation intervals for double precision floating points <ADD,MAX,MUL,MAD,DIV>Default 8,8,8,8,130
-ptx_opcode_initiation_sfu                    8 # Opcode initiation intervals for sfu instructionsDefault 8
-ptx_opcode_initiation_tensor                   64 # Opcode initiation intervals for tensor instructionsDefault 64
-cdp_latency         7200,8000,100,12000,1600 # CDP API latency <cudaStreamCreateWithFlags, cudaGetParameterBufferV2_init_perWarp, cudaGetParameterBufferV2_perKernel, cudaLaunchDeviceV2_init_perWarp, cudaLaunchDevicV2_perKernel>Default 7200,8000,100,12000,1600
-network_mode                           1 # Interconnection network mode
-inter_config_file   config_fermi_islip.icnt # Interconnection network config file
-icnt_in_buffer_limit                   64 # in_buffer_limit
-icnt_out_buffer_limit                   64 # out_buffer_limit
-icnt_subnets                           2 # subnets
-icnt_arbiter_algo                      1 # arbiter_algo
-icnt_verbose                           0 # inct_verbose
-icnt_grant_cycles                      1 # grant_cycles
-gpgpu_ptx_use_cuobjdump                    0 # Use cuobjdump to extract ptx and sass from binaries
-gpgpu_experimental_lib_support                    0 # Try to extract code from cuda libraries [Broken because of unknown cudaGetExportTable]
-checkpoint_option                      0 #  checkpointing flag (0 = no checkpoint)
-checkpoint_kernel                      1 #  checkpointing during execution of which kernel (1- 1st kernel)
-checkpoint_CTA                         0 #  checkpointing after # of CTA (< less than total CTA)
-resume_option                          0 #  resume flag (0 = no resume)
-resume_kernel                          0 #  Resume from which kernel (1= 1st kernel)
-resume_CTA                             0 #  resume from which CTA 
-checkpoint_CTA_t                       0 #  resume from which CTA 
-checkpoint_insn_Y                      0 #  resume from which CTA 
-gpgpu_ptx_convert_to_ptxplus                    0 # Convert SASS (native ISA) to ptxplus and run ptxplus
-gpgpu_ptx_force_max_capability                   60 # Force maximum compute capability
-gpgpu_ptx_inst_debug_to_file                    0 # Dump executed instructions' debug information to file
-gpgpu_ptx_inst_debug_file       inst_debug.txt # Executed instructions' debug output file
-gpgpu_ptx_inst_debug_thread_uid                    1 # Thread UID for executed instructions' debug output
-gpgpu_simd_model                       1 # 1 = post-dominator
-gpgpu_shader_core_pipeline              2048:32 # shader core pipeline config, i.e., {<nthread>:<warpsize>}
-gpgpu_tex_cache:l1  N:16:128:24,L:R:m:N:L,F:128:4,128:2 # per-shader L1 texture cache  (READ-ONLY) config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>:<rf>}
-gpgpu_const_cache:l1 N:128:64:2,L:R:f:N:L,A:2:64,4 # per-shader L1 constant memory cache  (READ-ONLY) config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>} 
-gpgpu_cache:il1     N:8:128:4,L:R:f:N:L,A:2:48,4 # shader L1 instruction cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>} 
-gpgpu_cache:dl1     N:64:128:6,L:L:m:N:H,A:128:8,8 # per-shader L1 data cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}
-gpgpu_l1_cache_write_ratio                    0 # L1D write ratio
-gpgpu_l1_banks                         1 # The number of L1 cache banks
-gpgpu_l1_banks_byte_interleaving                   32 # l1 banks byte interleaving granularity
-gpgpu_l1_banks_hashing_function                    0 # l1 banks hashing function
-gpgpu_l1_latency                       1 # L1 Hit Latency
-gpgpu_smem_latency                     3 # smem Latency
-gpgpu_cache:dl1PrefL1                 none # per-shader L1 data cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}
-gpgpu_cache:dl1PrefShared                 none # per-shader L1 data cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}
-gpgpu_gmem_skip_L1D                    1 # global memory access skip L1D cache (implements -Xptxas -dlcm=cg, default=no skip)
-gpgpu_perfect_mem                      0 # enable perfect memory mode (no cache miss)
-n_regfile_gating_group                    4 # group of lanes that should be read/written together)
-gpgpu_clock_gated_reg_file                    0 # enable clock gated reg file for power calculations
-gpgpu_clock_gated_lanes                    0 # enable clock gated lanes for power calculations
-gpgpu_shader_registers                65536 # Number of registers per shader core. Limits number of concurrent CTAs. (default 8192)
-gpgpu_registers_per_block                 8192 # Maximum number of registers per CTA. (default 8192)
-gpgpu_ignore_resources_limitation                    0 # gpgpu_ignore_resources_limitation (default 0)
-gpgpu_shader_cta                      32 # Maximum number of concurrent CTAs in shader (default 32)
-gpgpu_num_cta_barriers                   16 # Maximum number of named barriers per CTA (default 16)
-gpgpu_n_clusters                      20 # number of processing clusters
-gpgpu_n_cores_per_cluster                    1 # number of simd cores per cluster
-gpgpu_n_cluster_ejection_buffer_size                    8 # number of packets in ejection buffer
-gpgpu_n_ldst_response_buffer_size                    2 # number of response packets in ld/st unit ejection buffer
-gpgpu_shmem_per_block                49152 # Size of shared memory per thread block or CTA (default 48kB)
-gpgpu_shmem_size                   98304 # Size of shared memory per shader core (default 16kB)
-gpgpu_shmem_option                     0 # Option list of shared memory sizes
-gpgpu_unified_l1d_size                    0 # Size of unified data cache(L1D + shared memory) in KB
-gpgpu_adaptive_cache_config                    0 # adaptive_cache_config
-gpgpu_shmem_sizeDefault                16384 # Size of shared memory per shader core (default 16kB)
-gpgpu_shmem_size_PrefL1                16384 # Size of shared memory per shader core (default 16kB)
-gpgpu_shmem_size_PrefShared                16384 # Size of shared memory per shader core (default 16kB)
-gpgpu_shmem_num_banks                   32 # Number of banks in the shared memory in each shader core (default 16)
-gpgpu_shmem_limited_broadcast                    0 # Limit shared memory to do one broadcast per cycle (default on)
-gpgpu_shmem_warp_parts                    1 # Number of portions a warp is divided into for shared memory bank conflict check 
-gpgpu_mem_unit_ports                    1 # The number of memory transactions allowed per core cycle
-gpgpu_shmem_warp_parts                    1 # Number of portions a warp is divided into for shared memory bank conflict check 
-gpgpu_warpdistro_shader                   -1 # Specify which shader core to collect the warp size distribution from
-gpgpu_warp_issue_shader                    0 # Specify which shader core to collect the warp issue distribution from
-gpgpu_local_mem_map                    1 # Mapping from local memory space address to simulated GPU physical address space (default = enabled)
-gpgpu_num_reg_banks                   32 # Number of register banks (default = 8)
-gpgpu_reg_bank_use_warp_id                    0 # Use warp ID in mapping registers to banks (default = off)
-gpgpu_sub_core_model                    0 # Sub Core Volta/Pascal model (default = off)
-gpgpu_enable_specialized_operand_collector                    1 # enable_specialized_operand_collector
-gpgpu_operand_collector_num_units_sp                   20 # number of collector units (default = 4)
-gpgpu_operand_collector_num_units_dp                    0 # number of collector units (default = 0)
-gpgpu_operand_collector_num_units_sfu                    4 # number of collector units (default = 4)
-gpgpu_operand_collector_num_units_int                    0 # number of collector units (default = 0)
-gpgpu_operand_collector_num_units_tensor_core                    4 # number of collector units (default = 4)
-gpgpu_operand_collector_num_units_mem                    8 # number of collector units (default = 2)
-gpgpu_operand_collector_num_units_gen                    0 # number of collector units (default = 0)
-gpgpu_operand_collector_num_in_ports_sp                    4 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_dp                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_in_ports_sfu                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_int                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_in_ports_tensor_core                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_mem                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_gen                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_out_ports_sp                    4 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_dp                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_out_ports_sfu                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_int                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_out_ports_tensor_core                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_mem                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_gen                    0 # number of collector unit in ports (default = 0)
-gpgpu_coalesce_arch                   13 # Coalescing arch (GT200 = 13, Fermi = 20)
-gpgpu_num_sched_per_core                    2 # Number of warp schedulers per core
-gpgpu_max_insn_issue_per_warp                    2 # Max number of instructions that can be issued per warp in one cycle by scheduler (either 1 or 2)
-gpgpu_dual_issue_diff_exec_units                    1 # should dual issue use two different execution unit resources (Default = 1)
-gpgpu_simt_core_sim_order                    1 # Select the simulation order of cores in a cluster (0=Fix, 1=Round-Robin)
-gpgpu_pipeline_widths 4,0,0,1,1,4,0,0,1,1,6 # Pipeline widths ID_OC_SP,ID_OC_DP,ID_OC_INT,ID_OC_SFU,ID_OC_MEM,OC_EX_SP,OC_EX_DP,OC_EX_INT,OC_EX_SFU,OC_EX_MEM,EX_WB,ID_OC_TENSOR_CORE,OC_EX_TENSOR_CORE
-gpgpu_tensor_core_avail                    0 # Tensor Core Available (default=0)
-gpgpu_num_sp_units                     4 # Number of SP units (default=1)
-gpgpu_num_dp_units                     0 # Number of DP units (default=0)
-gpgpu_num_int_units                    0 # Number of INT units (default=0)
-gpgpu_num_sfu_units                    1 # Number of SF units (default=1)
-gpgpu_num_tensor_core_units                    0 # Number of tensor_core units (default=1)
-gpgpu_num_mem_units                    1 # Number if ldst units (default=1) WARNING: not hooked up to anything
-gpgpu_scheduler                      gto # Scheduler configuration: < lrr | gto | two_level_active > If two_level_active:<num_active_warps>:<inner_prioritization>:<outer_prioritization>For complete list of prioritization values see shader.h enum scheduler_prioritization_typeDefault: gto
-gpgpu_concurrent_kernel_sm                    0 # Support concurrent kernels on a SM (default = disabled)
-gpgpu_perfect_inst_const_cache                    0 # perfect inst and const cache mode, so all inst and const hits in the cache(default = disabled)
-gpgpu_inst_fetch_throughput                    1 # the number of fetched intruction per warp each cycle
-gpgpu_reg_file_port_throughput                    1 # the number ports of the register file
-specialized_unit_1         1,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_2       1,4,200,4,4,TEX # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_3     1,4,32,4,4,TENSOR # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_4         1,4,4,4,4,UDP # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_5         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_6         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_7         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_8         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-gpgpu_perf_sim_memcpy                    1 # Fill the L2 cache on memcpy
-gpgpu_simple_dram_model                    0 # simple_dram_model with fixed latency and BW
-gpgpu_dram_scheduler                    1 # 0 = fifo, 1 = FR-FCFS (defaul)
-gpgpu_dram_partition_queues              8:8:8:8 # i2$:$2d:d2$:$2i
-l2_ideal                               0 # Use a ideal L2 cache that always hit
-gpgpu_cache:dl2     N:64:128:16,L:B:m:W:L,A:1024:1024,4:0,32 # unified banked L2 data cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>}
-gpgpu_cache:dl2_texture_only                    0 # L2 cache used for texture only
-gpgpu_n_mem                            8 # number of memory modules (e.g. memory controllers) in gpu
-gpgpu_n_sub_partition_per_mchannel                    2 # number of memory subpartition in each memory module
-gpgpu_n_mem_per_ctrlr                    1 # number of memory chips per memory controller
-gpgpu_memlatency_stat                   14 # track and display latency statistics 0x2 enables MC, 0x4 enables queue logs
-gpgpu_frfcfs_dram_sched_queue_size                   64 # 0 = unlimited (default); # entries per chip
-gpgpu_dram_return_queue_size                  116 # 0 = unlimited (default); # entries per chip
-gpgpu_dram_buswidth                    4 # default = 4 bytes (8 bytes per cycle at DDR)
-gpgpu_dram_burst_length                    8 # Burst length of each DRAM request (default = 4 data bus cycle)
-dram_data_command_freq_ratio                    4 # Frequency ratio between DRAM data bus and command bus (default = 2 times, i.e. DDR)
-gpgpu_dram_timing_opt nbk=16:CCD=2:RRD=6:RCD=12:RAS=28:RP=12:RC=40: CL=12:WL=4:CDLR=5:WR=12:nbkgrp=1:CCDL=0:RTPL=0 # DRAM timing parameters = {nbk:tCCD:tRRD:tRCD:tRAS:tRP:tRC:CL:WL:tCDLR:tWR:nbkgrp:tCCDL:tRTPL}
-gpgpu_l2_rop_latency                  120 # ROP queue latency (default 85)
-dram_latency                         100 # DRAM latency (default 30)
-dram_dual_bus_interface                    0 # dual_bus_interface (default = 0) 
-dram_bnk_indexing_policy                    0 # dram_bnk_indexing_policy (0 = normal indexing, 1 = Xoring with the higher bits) (Default = 0)
-dram_bnkgrp_indexing_policy                    0 # dram_bnkgrp_indexing_policy (0 = take higher bits, 1 = take lower bits) (Default = 0)
-dram_seperate_write_queue_enable                    0 # Seperate_Write_Queue_Enable
-dram_write_queue_size             32:28:16 # Write_Queue_Size
-dram_elimnate_rw_turnaround                    0 # elimnate_rw_turnaround i.e set tWTR and tRTW = 0
-icnt_flit_size                        32 # icnt_flit_size
-gpgpu_mem_addr_mapping dramid@8;00000000.00000000.00000000.00000000.0000RRRR.RRRRRRRR.RBBBCCCC.BCCSSSSS # mapping memory address to dram model {dramid@<start bit>;<memory address map>}
-gpgpu_mem_addr_test                    0 # run sweep test to check address mapping for aliased address
-gpgpu_mem_address_mask                    1 # 0 = old addressing mask, 1 = new addressing mask, 2 = new add. mask + flipped bank sel and chip sel bits
-gpgpu_memory_partition_indexing                    0 # 0 = no indexing, 1 = bitwise xoring, 2 = IPoly, 3 = custom indexing
-accelwattch_xml_file accelwattch_sass_sim.xml # AccelWattch XML file
-power_simulation_enabled                    0 # Turn on power simulator (1=On, 0=Off)
-power_per_cycle_dump                    0 # Dump detailed power output each cycle
-hw_perf_file_name            hw_perf.csv # Hardware Performance Statistics file
-hw_perf_bench_name                       # Kernel Name in Hardware Performance Statistics file
-power_simulation_mode                    0 # Switch performance counter input for power simulation (0=Sim, 1=HW, 2=HW-Sim Hybrid)
-dvfs_enabled                           0 # Turn on DVFS for power model
-aggregate_power_stats                    0 # Accumulate power across all kernels
-accelwattch_hybrid_perfsim_L1_RH                    0 # Get L1 Read Hits for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L1_RM                    0 # Get L1 Read Misses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L1_WH                    0 # Get L1 Write Hits for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L1_WM                    0 # Get L1 Write Misses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L2_RH                    0 # Get L2 Read Hits for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L2_RM                    0 # Get L2 Read Misses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L2_WH                    0 # Get L2 Write Hits for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L2_WM                    0 # Get L2 Write Misses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_CC_ACC                    0 # Get Constant Cache Acesses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_SHARED_ACC                    0 # Get Shared Memory Acesses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_DRAM_RD                    0 # Get DRAM Reads for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_DRAM_WR                    0 # Get DRAM Writes for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_NOC                    0 # Get Interconnect Acesses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_PIPE_DUTY                    0 # Get Pipeline Duty Cycle Acesses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_NUM_SM_IDLE                    0 # Get Number of Idle SMs for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_CYCLES                    0 # Get Executed Cycles for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_VOLTAGE                    0 # Get Chip Voltage for Accelwattch-Hybrid from Accel-Sim
-power_trace_enabled                    0 # produce a file for the power trace (1=On, 0=Off)
-power_trace_zlevel                     6 # Compression level of the power trace output log (0=no comp, 9=highest)
-steady_power_levels_enabled                    0 # produce a file for the steady power levels (1=On, 0=Off)
-steady_state_definition                  8:4 # allowed deviation:number of samples
-gpgpu_max_cycle                        0 # terminates gpu simulation early (0 = no limit)
-gpgpu_max_insn                         0 # terminates gpu simulation early (0 = no limit)
-gpgpu_max_cta                          0 # terminates gpu simulation early (0 = no limit)
-gpgpu_max_completed_cta                    0 # terminates gpu simulation early (0 = no limit)
-gpgpu_runtime_stat                   500 # display runtime statistics such as dram utilization {<freq>:<flag>}
-liveness_message_freq                    1 # Minimum number of seconds between simulation liveness messages (0 = always print)
-gpgpu_compute_capability_major                    7 # Major compute capability version number
-gpgpu_compute_capability_minor                    0 # Minor compute capability version number
-gpgpu_flush_l1_cache                    0 # Flush L1 cache at the end of each kernel call
-gpgpu_flush_l2_cache                    0 # Flush L2 cache at the end of each kernel call
-gpgpu_deadlock_detect                    1 # Stop the simulation at deadlock (1=on (default), 0=off)
-gpgpu_ptx_instruction_classification                    0 # if enabled will classify ptx instruction types per kernel (Max 255 kernels now)
-gpgpu_ptx_sim_mode                     0 # Select between Performance (default) or Functional simulation (1)
-gpgpu_clock_domains 1607.0:1607.0:1607.0:2500.0 # Clock Domain Frequencies in MhZ {<Core Clock>:<ICNT Clock>:<L2 Clock>:<DRAM Clock>}
-gpgpu_max_concurrent_kernel                   32 # maximum kernels that can run concurrently on GPU, set this value according to max resident grids for your compute capability
-gpgpu_cflog_interval                    0 # Interval between each snapshot in control flow logger
-visualizer_enabled                     0 # Turn on visualizer output (1=On, 0=Off)
-visualizer_outputfile                 NULL # Specifies the output log file for visualizer
-visualizer_zlevel                      6 # Compression level of the visualizer output log (0=no comp, 9=highest)
-gpgpu_stack_size_limit                 1024 # GPU thread stack size
-gpgpu_heap_size_limit              8388608 # GPU malloc heap size 
-gpgpu_runtime_sync_depth_limit                    2 # GPU device runtime synchronize depth
-gpgpu_runtime_pending_launch_count_limit                 2048 # GPU device runtime pending launch count
-trace_enabled                          0 # Turn on traces
-trace_components                    none # comma seperated list of traces to enable. Complete list found in trace_streams.tup. Default none
-trace_sampling_core                    0 # The core which is printed using CORE_DPRINTF. Default 0
-trace_sampling_memory_partition                   -1 # The memory partition which is printed using MEMPART_DPRINTF. Default -1 (i.e. all)
-enable_ptx_file_line_stats                    1 # Turn on PTX source line statistic profiling. (1 = On)
-ptx_line_stats_filename gpgpu_inst_stats.txt # Output file for PTX source line statistics.
-gpgpu_kernel_launch_latency                    0 # Kernel launch latency in cycles. Default: 0
-gpgpu_cdp_enabled                      0 # Turn on CDP
-gpgpu_TB_launch_latency                    0 # thread block launch latency in cycles. Default: 0
-trace               /home/roman/dev/box/test-apps/vectoradd/traces/vectoradd-100-32-trace/kernelslist.g # traces kernel filetraces kernel file directory
-trace_opcode_latency_initiation_int                  2,2 # Opcode latencies and initiation for integers in trace driven mode <latency,initiation>
-trace_opcode_latency_initiation_sp                  2,1 # Opcode latencies and initiation for sp in trace driven mode <latency,initiation>
-trace_opcode_latency_initiation_dp                64,64 # Opcode latencies and initiation for dp in trace driven mode <latency,initiation>
-trace_opcode_latency_initiation_sfu                 21,8 # Opcode latencies and initiation for sfu in trace driven mode <latency,initiation>
-trace_opcode_latency_initiation_tensor                32,32 # Opcode latencies and initiation for tensor in trace driven mode <latency,initiation>
-trace_opcode_latency_initiation_spec_op_1                  4,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_2                200,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_3                32,32 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_4                  4,1 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_5                  4,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_6                  4,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_7                  4,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_8                  4,4 # specialized unit config <latency,initiation>
DRAM Timing Options:
nbk                                    16 # number of banks
CCD                                     2 # column to column delay
RRD                                     6 # minimal delay between activation of rows in different banks
RCD                                    12 # row to column delay
RAS                                    28 # time needed to activate row
RP                                     12 # time needed to precharge (deactivate) row
RC                                     40 # row cycle time
CDLR                                    5 # switching from write to read (changes tWTR)
WR                                     12 # last data-in to row precharge
CL                                     12 # CAS latency
WL                                      4 # Write latency
nbkgrp                                  1 # number of bank groups
CCDL                                    0 # column to column delay between accesses to different bank groups
RTPL                                    0 # read to precharge delay between accesses to different bank groups
Total number of memory sub partition = 16
addr_dec_mask[CHIP]  = 0000000000000700 	high:11 low:8
addr_dec_mask[BK]    = 0000000000038080 	high:18 low:7
addr_dec_mask[ROW]   = 000000007ffc0000 	high:31 low:18
addr_dec_mask[COL]   = 000000000000787f 	high:15 low:0
addr_dec_mask[BURST] = 000000000000001f 	high:5 low:0
sub_partition_id_mask = 0000000000000080
GPGPU-Sim uArch: clock freqs: 1607000000.000000:1607000000.000000:1607000000.000000:2500000000.000000
GPGPU-Sim uArch: clock periods: 0.00000000062227753578:0.00000000062227753578:0.00000000062227753578:0.00000000040000000000
*** Initializing Memory Statistics ***
GPGPU-Sim uArch: interconnect node map (shaderID+MemID to icntID)
GPGPU-Sim uArch: Memory nodes ID start from index: 20
GPGPU-Sim uArch:    0   1   2   3   4   5   6
GPGPU-Sim uArch:    7   8   9  10  11  12  13
GPGPU-Sim uArch:   14  15  16  17  18  19  20
GPGPU-Sim uArch:   21  22  23  24  25  26  27
GPGPU-Sim uArch:   28  29  30  31  32  33  34
GPGPU-Sim uArch:   35  36  37  38  39  40  41
GPGPU-Sim uArch:   42  43  44  45  46  47  48
GPGPU-Sim uArch:   49
GPGPU-Sim uArch: interconnect node reverse map (icntID to shaderID+MemID)
GPGPU-Sim uArch: Memory nodes start from ID: 20
GPGPU-Sim uArch:    0   1   2   3   4   5   6
GPGPU-Sim uArch:    7   8   9  10  11  12  13
GPGPU-Sim uArch:   14  15  16  17  18  19  20
GPGPU-Sim uArch:   21  22  23  24  25  26  27
GPGPU-Sim uArch:   28  29  30  31  32  33  34
GPGPU-Sim uArch:   35  36  37  38  39  40  41
GPGPU-Sim uArch:   42  43  44  45  46  47  48
GPGPU-Sim uArch:   49
GPGPU-Sim uArch: performance model initialization complete.
initialization complete
launching memcpy command : MemcpyHtoD,0x00007f2b2d500000,400
tag_array::fill(760217600)
tag_array::probe(760217600)
cache_config::set_index(47513600)
cache_config::hash_function(47513600)
tag_array::fill(760217632)
tag_array::probe(760217632)
cache_config::set_index(47513632)
cache_config::hash_function(47513632)
tag_array::fill(760217664)
tag_array::probe(760217664)
cache_config::set_index(47513664)
cache_config::hash_function(47513664)
tag_array::fill(760217696)
tag_array::probe(760217696)
cache_config::set_index(47513696)
cache_config::hash_function(47513696)
tag_array::fill(760217728)
tag_array::probe(760217728)
cache_config::set_index(47513600)
cache_config::hash_function(47513600)
tag_array::fill(760217760)
tag_array::probe(760217760)
cache_config::set_index(47513632)
cache_config::hash_function(47513632)
tag_array::fill(760217792)
tag_array::probe(760217792)
cache_config::set_index(47513664)
cache_config::hash_function(47513664)
tag_array::fill(760217824)
tag_array::probe(760217824)
cache_config::set_index(47513696)
cache_config::hash_function(47513696)
tag_array::fill(760217856)
tag_array::probe(760217856)
cache_config::set_index(47513600)
cache_config::hash_function(47513600)
tag_array::fill(760217888)
tag_array::probe(760217888)
cache_config::set_index(47513632)
cache_config::hash_function(47513632)
tag_array::fill(760217920)
tag_array::probe(760217920)
cache_config::set_index(47513664)
cache_config::hash_function(47513664)
tag_array::fill(760217952)
tag_array::probe(760217952)
cache_config::set_index(47513696)
cache_config::hash_function(47513696)
tag_array::fill(760217984)
tag_array::probe(760217984)
cache_config::set_index(47513600)
cache_config::hash_function(47513600)
launching memcpy command : MemcpyHtoD,0x00007f2b2d500200,400
tag_array::fill(760218112)
tag_array::probe(760218112)
cache_config::set_index(47513600)
cache_config::hash_function(47513600)
tag_array::fill(760218144)
tag_array::probe(760218144)
cache_config::set_index(47513632)
cache_config::hash_function(47513632)
tag_array::fill(760218176)
tag_array::probe(760218176)
cache_config::set_index(47513664)
cache_config::hash_function(47513664)
tag_array::fill(760218208)
tag_array::probe(760218208)
cache_config::set_index(47513696)
cache_config::hash_function(47513696)
tag_array::fill(760218240)
tag_array::probe(760218240)
cache_config::set_index(47513600)
cache_config::hash_function(47513600)
tag_array::fill(760218272)
tag_array::probe(760218272)
cache_config::set_index(47513632)
cache_config::hash_function(47513632)
tag_array::fill(760218304)
tag_array::probe(760218304)
cache_config::set_index(47513664)
cache_config::hash_function(47513664)
tag_array::fill(760218336)
tag_array::probe(760218336)
cache_config::set_index(47513696)
cache_config::hash_function(47513696)
tag_array::fill(760218368)
tag_array::probe(760218368)
cache_config::set_index(47513600)
cache_config::hash_function(47513600)
tag_array::fill(760218400)
tag_array::probe(760218400)
cache_config::set_index(47513632)
cache_config::hash_function(47513632)
tag_array::fill(760218432)
tag_array::probe(760218432)
cache_config::set_index(47513664)
cache_config::hash_function(47513664)
tag_array::fill(760218464)
tag_array::probe(760218464)
cache_config::set_index(47513696)
cache_config::hash_function(47513696)
tag_array::fill(760218496)
tag_array::probe(760218496)
cache_config::set_index(47513600)
cache_config::hash_function(47513600)
launching memcpy command : MemcpyHtoD,0x00007f2b2d500400,400
tag_array::fill(760218624)
tag_array::probe(760218624)
cache_config::set_index(47513600)
cache_config::hash_function(47513600)
tag_array::fill(760218656)
tag_array::probe(760218656)
cache_config::set_index(47513632)
cache_config::hash_function(47513632)
tag_array::fill(760218688)
tag_array::probe(760218688)
cache_config::set_index(47513664)
cache_config::hash_function(47513664)
tag_array::fill(760218720)
tag_array::probe(760218720)
cache_config::set_index(47513696)
cache_config::hash_function(47513696)
tag_array::fill(760218752)
tag_array::probe(760218752)
cache_config::set_index(47513600)
cache_config::hash_function(47513600)
tag_array::fill(760218784)
tag_array::probe(760218784)
cache_config::set_index(47513632)
cache_config::hash_function(47513632)
tag_array::fill(760218816)
tag_array::probe(760218816)
cache_config::set_index(47513664)
cache_config::hash_function(47513664)
tag_array::fill(760218848)
tag_array::probe(760218848)
cache_config::set_index(47513696)
cache_config::hash_function(47513696)
tag_array::fill(760218880)
tag_array::probe(760218880)
cache_config::set_index(47513600)
cache_config::hash_function(47513600)
tag_array::fill(760218912)
tag_array::probe(760218912)
cache_config::set_index(47513632)
cache_config::hash_function(47513632)
tag_array::fill(760218944)
tag_array::probe(760218944)
cache_config::set_index(47513664)
cache_config::hash_function(47513664)
tag_array::fill(760218976)
tag_array::probe(760218976)
cache_config::set_index(47513696)
cache_config::hash_function(47513696)
tag_array::fill(760219008)
tag_array::probe(760219008)
cache_config::set_index(47513600)
cache_config::hash_function(47513600)
Processing kernel /home/roman/dev/box/test-apps/vectoradd/traces/vectoradd-100-32-trace/kernel-1.traceg
-kernel name = _Z6vecAddIfEvPT_S1_S1_i
-kernel id = 1
-grid dim = (1,1,1)
-block dim = (1024,1,1)
-shmem = 0
-nregs = 8
-binary version = 61
-cuda stream id = 0
-shmem base_addr = 0x00007f2b5a000000
-local mem base_addr = 0x00007f2b58000000
-nvbit version = 1.5.5
-accelsim tracer version = 4
-enable lineinfo = 0
Header info loaded for kernel command : /home/roman/dev/box/test-apps/vectoradd/traces/vectoradd-100-32-trace/kernel-1.traceg
launching kernel name: _Z6vecAddIfEvPT_S1_S1_i uid: 1
GPGPU-Sim uArch: Shader 0 bind to kernel 1 '_Z6vecAddIfEvPT_S1_S1_i'
GPGPU-Sim uArch: CTA/core = 2, limited by: threads
thread block = 0,0,0
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(251658240)
cache_config::hash_function(251658240)
tag_array::access(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(251658240)
cache_config::hash_function(251658240)
tag_array::fill(14, 4026531840)
mshr_table::mark_ready(4026531840, 0)
tag_array::fill(4026531840)
tag_array::probe(4026531840)
cache_config::set_index(4026531840)
cache_config::hash_function(4026531840)
mshr_table::mark_ready(4026531840, 0)
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [0 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
====> instruction SHL parsed as OP_SHL (136) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = SHL (OP_SHL, 136)
====> instruction SHR parsed as OP_SHR (50) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = SHR (OP_SHR, 50)
====> instruction IADD parsed as OP_IADD (31) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD (OP_IADD, 31)
====> instruction IADD parsed as OP_IADD (31) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD.X (OP_IADD, 31)
====> instruction IADD parsed as OP_IADD (31) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD (OP_IADD, 31)
====> instruction LDG parsed as OP_LDG (72) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = LDG.E.CG (OP_LDG, 72)
==>> ROMAN: warp 1 trace done 0 (15/23) functional done 0
==>> ROMAN: warp 1 trace done 0 (15/23) functional done 0
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [0 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
====> instruction SHL parsed as OP_SHL (136) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = SHL (OP_SHL, 136)
====> instruction SHR parsed as OP_SHR (50) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = SHR (OP_SHR, 50)
====> instruction IADD parsed as OP_IADD (31) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD (OP_IADD, 31)
====> instruction IADD parsed as OP_IADD (31) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD.X (OP_IADD, 31)
====> instruction IADD parsed as OP_IADD (31) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD (OP_IADD, 31)
====> instruction LDG parsed as OP_LDG (72) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = LDG.E.CG (OP_LDG, 72)
==>> ROMAN: warp 2 trace done 0 (15/23) functional done 0
==>> ROMAN: warp 2 trace done 0 (15/23) functional done 0
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [28 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
====> instruction SHL parsed as OP_SHL (136) [4 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = SHL (OP_SHL, 136)
====> instruction SHR parsed as OP_SHR (50) [4 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = SHR (OP_SHR, 50)
====> instruction IADD parsed as OP_IADD (31) [4 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD (OP_IADD, 31)
====> instruction IADD parsed as OP_IADD (31) [4 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD.X (OP_IADD, 31)
====> instruction IADD parsed as OP_IADD (31) [4 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD (OP_IADD, 31)
====> instruction LDG parsed as OP_LDG (72) [4 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = LDG.E.CG (OP_LDG, 72)
==>> ROMAN: warp 3 trace done 0 (15/23) functional done 0
==>> ROMAN: warp 3 trace done 0 (15/23) functional done 0
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 4 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 5 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 6 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 7 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 8 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 9 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 10 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 11 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 12 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 13 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 14 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 15 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 16 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 17 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 18 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 19 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 20 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 21 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 22 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 23 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 24 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 25 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 26 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 27 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 28 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 29 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 30 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 31 trace done 1 (9/9) functional done 1
====> instruction MOV parsed as OP_MOV (60) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = MOV (OP_MOV, 60)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction S2R parsed as OP_S2R (124) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = S2R (OP_S2R, 124)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.MRG (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD (OP_XMAD, 137)
====> instruction XMAD parsed as OP_XMAD (137) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = XMAD.PSL.CBCC (OP_XMAD, 137)
====> instruction ISETP parsed as OP_ISETP (43) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = ISETP.GE.AND (OP_ISETP, 43)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [0 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
====> instruction SHL parsed as OP_SHL (136) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = SHL (OP_SHL, 136)
====> instruction SHR parsed as OP_SHR (50) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = SHR (OP_SHR, 50)
====> instruction IADD parsed as OP_IADD (31) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD (OP_IADD, 31)
====> instruction IADD parsed as OP_IADD (31) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD.X (OP_IADD, 31)
====> instruction IADD parsed as OP_IADD (31) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD (OP_IADD, 31)
====> instruction LDG parsed as OP_LDG (72) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = LDG.E.CG (OP_LDG, 72)
tag_array::probe(4026531968)
cache_config::set_index(4026531968)
cache_config::hash_function(4026531968)
tag_array::access(4026531968)
tag_array::probe(4026531968)
cache_config::set_index(4026531968)
cache_config::hash_function(4026531968)
==>> ROMAN: warp 0 trace done 0 (15/23) functional done 0
==>> ROMAN: warp 0 trace done 0 (15/23) functional done 0
tag_array::probe(4026531968)
cache_config::set_index(4026531968)
cache_config::hash_function(4026531968)
tag_array::access(4026531968)
tag_array::probe(4026531968)
cache_config::set_index(4026531968)
cache_config::hash_function(4026531968)
tag_array::probe(4026531968)
cache_config::set_index(4026531968)
cache_config::hash_function(4026531968)
tag_array::access(4026531968)
tag_array::probe(4026531968)
cache_config::set_index(4026531968)
cache_config::hash_function(4026531968)
tag_array::probe(4026531968)
cache_config::set_index(4026531968)
cache_config::hash_function(4026531968)
tag_array::access(4026531968)
tag_array::probe(4026531968)
cache_config::set_index(4026531968)
cache_config::hash_function(4026531968)
tag_array::probe(139823420539008)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
tag_array::access(139823420539008)
tag_array::probe(139823420539008)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
tag_array::probe(139823420539136)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
tag_array::access(139823420539136)
tag_array::probe(139823420539136)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
tag_array::probe(139823420539264)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
tag_array::access(139823420539264)
tag_array::probe(139823420539264)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
tag_array::probe(4026531968)
cache_config::set_index(251658240)
cache_config::hash_function(251658240)
tag_array::access(4026531968)
tag_array::probe(4026531968)
cache_config::set_index(251658240)
cache_config::hash_function(251658240)
tag_array::probe(139823420538880)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
tag_array::access(139823420538880)
tag_array::probe(139823420538880)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
tag_array::fill(14, 139823420539008)
mshr_table::mark_ready(139823420539008, 0)
tag_array::fill(14, 139823420539136)
mshr_table::mark_ready(139823420539136, 0)
tag_array::fill(14, 139823420539264)
mshr_table::mark_ready(139823420539264, 0)
tag_array::fill(13, 4026531968)
mshr_table::mark_ready(4026531968, 0)
tag_array::fill(13, 139823420538880)
mshr_table::mark_ready(139823420538880, 0)
tag_array::fill(4026531968)
tag_array::probe(4026531968)
cache_config::set_index(4026531968)
cache_config::hash_function(4026531968)
mshr_table::mark_ready(4026531968, 0)
====> instruction IADD parsed as OP_IADD (31) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD.X (OP_IADD, 31)
====> instruction LDG parsed as OP_LDG (72) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = LDG.E.CG (OP_LDG, 72)
====> instruction IADD parsed as OP_IADD (31) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD (OP_IADD, 31)
====> instruction IADD parsed as OP_IADD (31) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD.X (OP_IADD, 31)
====> instruction FADD parsed as OP_FADD (1) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = FADD (OP_FADD, 1)
====> instruction STG parsed as OP_STG (76) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = STG.E (OP_STG, 76)
==>> ROMAN: warp 1 trace done 0 (21/23) functional done 0
====> instruction IADD parsed as OP_IADD (31) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD.X (OP_IADD, 31)
====> instruction LDG parsed as OP_LDG (72) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = LDG.E.CG (OP_LDG, 72)
====> instruction IADD parsed as OP_IADD (31) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD (OP_IADD, 31)
====> instruction IADD parsed as OP_IADD (31) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD.X (OP_IADD, 31)
====> instruction FADD parsed as OP_FADD (1) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = FADD (OP_FADD, 1)
====> instruction STG parsed as OP_STG (76) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = STG.E (OP_STG, 76)
==>> ROMAN: warp 1 trace done 0 (21/23) functional done 0
====> instruction IADD parsed as OP_IADD (31) [4 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD.X (OP_IADD, 31)
====> instruction LDG parsed as OP_LDG (72) [4 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = LDG.E.CG (OP_LDG, 72)
====> instruction IADD parsed as OP_IADD (31) [4 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD (OP_IADD, 31)
====> instruction IADD parsed as OP_IADD (31) [4 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD.X (OP_IADD, 31)
====> instruction FADD parsed as OP_FADD (1) [4 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = FADD (OP_FADD, 1)
====> instruction STG parsed as OP_STG (76) [4 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = STG.E (OP_STG, 76)
==>> ROMAN: warp 2 trace done 0 (21/23) functional done 0
====> instruction IADD parsed as OP_IADD (31) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD.X (OP_IADD, 31)
====> instruction LDG parsed as OP_LDG (72) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = LDG.E.CG (OP_LDG, 72)
====> instruction IADD parsed as OP_IADD (31) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD (OP_IADD, 31)
====> instruction IADD parsed as OP_IADD (31) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = IADD.X (OP_IADD, 31)
====> instruction FADD parsed as OP_FADD (1) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = FADD (OP_FADD, 1)
====> instruction STG parsed as OP_STG (76) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = STG.E (OP_STG, 76)
tag_array::probe(4026531968)
cache_config::set_index(4026531968)
cache_config::hash_function(4026531968)
tag_array::access(4026531968)
tag_array::probe(4026531968)
cache_config::set_index(4026531968)
cache_config::hash_function(4026531968)
==>> ROMAN: warp 3 trace done 0 (21/23) functional done 0
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 2 trace done 0 (21/23) functional done 0
==>> ROMAN: warp 1 trace done 1 (23/23) functional done 1
tag_array::probe(4026531968)
cache_config::set_index(4026531968)
cache_config::hash_function(4026531968)
tag_array::access(4026531968)
tag_array::probe(4026531968)
cache_config::set_index(4026531968)
cache_config::hash_function(4026531968)
==>> ROMAN: warp 3 trace done 0 (21/23) functional done 0
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
tag_array::probe(4026531968)
cache_config::set_index(4026531968)
cache_config::hash_function(4026531968)
tag_array::access(4026531968)
tag_array::probe(4026531968)
cache_config::set_index(4026531968)
cache_config::hash_function(4026531968)
==>> ROMAN: warp 2 trace done 1 (23/23) functional done 1
====> instruction NOP parsed as OP_NOP (121) [4 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [4 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 3 trace done 1 (23/23) functional done 1
==>> ROMAN: warp 0 trace done 0 (21/23) functional done 0
==>> ROMAN: warp 0 trace done 0 (21/23) functional done 0
tag_array::probe(4026531968)
cache_config::set_index(4026531968)
cache_config::hash_function(4026531968)
tag_array::access(4026531968)
tag_array::probe(4026531968)
cache_config::set_index(4026531968)
cache_config::hash_function(4026531968)
====> instruction NOP parsed as OP_NOP (121) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = NOP (OP_NOP, 121)
====> instruction EXIT parsed as OP_EXIT (104) [32 active threads]
====> trace_shd_warp_t::get_next_trace_inst(): opcode = EXIT (OP_EXIT, 104)
==>> ROMAN: warp 0 trace done 1 (23/23) functional done 1
tag_array::probe(139823420539520)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
tag_array::access(139823420539520)
tag_array::probe(139823420539520)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
tag_array::probe(139823420540032)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
data_cache::send_write_request(...)
tag_array::access(139823420540032)
tag_array::probe(139823420540032)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
tag_array::probe(139823420539648)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
tag_array::access(139823420539648)
tag_array::probe(139823420539648)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
tag_array::probe(139823420539776)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
tag_array::access(139823420539776)
tag_array::probe(139823420539776)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
tag_array::probe(139823420540160)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
data_cache::send_write_request(...)
tag_array::access(139823420540160)
tag_array::probe(139823420540160)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
tag_array::probe(139823420540288)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
data_cache::send_write_request(...)
tag_array::access(139823420540288)
tag_array::probe(139823420540288)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
tag_array::probe(139823420539392)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
tag_array::access(139823420539392)
tag_array::probe(139823420539392)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
tag_array::probe(139823420539904)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
data_cache::send_write_request(...)
tag_array::access(139823420539904)
tag_array::probe(139823420539904)
cache_config::set_index(8738963783680)
cache_config::hash_function(8738963783680)
tag_array::fill(14, 139823420539520)
mshr_table::mark_ready(139823420539520, 0)
tag_array::fill(14, 139823420539648)
mshr_table::mark_ready(139823420539648, 0)
tag_array::fill(14, 139823420539776)
mshr_table::mark_ready(139823420539776, 0)
tag_array::fill(14, 139823420540032)
mshr_table::mark_ready(139823420540032, 0)
tag_array::fill(14, 139823420539392)
mshr_table::mark_ready(139823420539392, 0)
tag_array::fill(14, 139823420540160)
mshr_table::mark_ready(139823420540160, 0)
tag_array::fill(14, 139823420540288)
mshr_table::mark_ready(139823420540288, 0)
tag_array::fill(14, 139823420539904)
mshr_table::mark_ready(139823420539904, 0)
kernel_name = _Z6vecAddIfEvPT_S1_S1_i 
kernel_launch_uid = 1 
gpu_sim_cycle = 868
gpu_sim_insn = 1324
gpu_ipc =       1.5253
gpu_tot_sim_cycle = 868
gpu_tot_sim_insn = 1324
gpu_tot_ipc =       1.5253
gpu_tot_issued_cta = 1
gpu_occupancy = 21.3445% 
gpu_tot_occupancy = 21.3445% 
max_total_param_size = 0
gpu_stall_dramfull = 0
gpu_stall_icnt2sh    = 0
partiton_level_parallism =       0.0161
partiton_level_parallism_total  =       0.0161
partiton_level_parallism_util =       1.0000
partiton_level_parallism_util_total  =       1.0000
L2_BW  =       0.8294 GB/Sec
L2_BW_total  =       0.8294 GB/Sec
gpu_total_sim_rate=1324

========= Core cache stats =========
L1I_cache:
	L1I_total_cache_accesses = 40
	L1I_total_cache_misses = 36
	L1I_total_cache_miss_rate = 0.9000
	L1I_total_cache_pending_hits = 0
	L1I_total_cache_reservation_fails = 0
L1D_cache:
	L1D_cache_core[0]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[1]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[2]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[3]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[4]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[5]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[6]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[7]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[8]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[9]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[10]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[11]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[12]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[13]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[14]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[15]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[16]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[17]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[18]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[19]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_total_cache_accesses = 0
	L1D_total_cache_misses = 0
	L1D_total_cache_pending_hits = 0
	L1D_total_cache_reservation_fails = 0
	L1D_cache_data_port_util = 0.000
	L1D_cache_fill_port_util = 0.000
L1C_cache:
	L1C_total_cache_accesses = 0
	L1C_total_cache_misses = 0
	L1C_total_cache_pending_hits = 0
	L1C_total_cache_reservation_fails = 0
L1T_cache:
	L1T_total_cache_accesses = 0
	L1T_total_cache_misses = 0
	L1T_total_cache_pending_hits = 0
	L1T_total_cache_reservation_fails = 0

Total_core_cache_stats:
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][HIT] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][MISS] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_R][HIT] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_R][MISS] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_R][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_R][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[CONST_ACC_R][HIT] = 0
	Total_core_cache_stats_breakdown[CONST_ACC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[CONST_ACC_R][MISS] = 0
	Total_core_cache_stats_breakdown[CONST_ACC_R][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[CONST_ACC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[CONST_ACC_R][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[TEXTURE_ACC_R][HIT] = 0
	Total_core_cache_stats_breakdown[TEXTURE_ACC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[TEXTURE_ACC_R][MISS] = 0
	Total_core_cache_stats_breakdown[TEXTURE_ACC_R][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[TEXTURE_ACC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[TEXTURE_ACC_R][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][HIT] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][MISS] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_W][HIT] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_W][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_W][MISS] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_W][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_W][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_W][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[L1_WRBK_ACC][HIT] = 0
	Total_core_cache_stats_breakdown[L1_WRBK_ACC][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[L1_WRBK_ACC][MISS] = 0
	Total_core_cache_stats_breakdown[L1_WRBK_ACC][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[L1_WRBK_ACC][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[L1_WRBK_ACC][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[L2_WRBK_ACC][HIT] = 0
	Total_core_cache_stats_breakdown[L2_WRBK_ACC][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[L2_WRBK_ACC][MISS] = 0
	Total_core_cache_stats_breakdown[L2_WRBK_ACC][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[L2_WRBK_ACC][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[L2_WRBK_ACC][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[INST_ACC_R][HIT] = 4
	Total_core_cache_stats_breakdown[INST_ACC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[INST_ACC_R][MISS] = 36
	Total_core_cache_stats_breakdown[INST_ACC_R][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[INST_ACC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[INST_ACC_R][MSHR_HIT] = 34
	Total_core_cache_stats_breakdown[L1_WR_ALLOC_R][HIT] = 0
	Total_core_cache_stats_breakdown[L1_WR_ALLOC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[L1_WR_ALLOC_R][MISS] = 0
	Total_core_cache_stats_breakdown[L1_WR_ALLOC_R][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[L1_WR_ALLOC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[L1_WR_ALLOC_R][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[L2_WR_ALLOC_R][HIT] = 0
	Total_core_cache_stats_breakdown[L2_WR_ALLOC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[L2_WR_ALLOC_R][MISS] = 0
	Total_core_cache_stats_breakdown[L2_WR_ALLOC_R][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[L2_WR_ALLOC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[L2_WR_ALLOC_R][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[INST_ACC_R][TOTAL_ACCESS] = 40

Total_core_cache_fail_stats:
ctas_completed 1, Shader 0 warp_id issue ditsribution:
warp_id:
0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 
distro:
6, 6, 6, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
gpgpu_n_tot_thrd_icount = 1536
gpgpu_n_tot_w_icount = 48
gpgpu_n_stall_shd_mem = 5
gpgpu_n_mem_read_local = 0
gpgpu_n_mem_write_local = 0
gpgpu_n_mem_read_global = 8
gpgpu_n_mem_write_global = 4
gpgpu_n_mem_texture = 0
gpgpu_n_mem_const = 0
gpgpu_n_load_insn  = 200
gpgpu_n_store_insn = 100
gpgpu_n_shmem_insn = 0
gpgpu_n_sstarr_insn = 0
gpgpu_n_tex_insn = 0
gpgpu_n_const_mem_insn = 0
gpgpu_n_param_mem_insn = 0
gpgpu_n_shmem_bkconflict = 0
gpgpu_n_cache_bkconflict = 0
gpgpu_n_intrawarp_mshr_merge = 0
gpgpu_n_cmem_portconflict = 0
gpgpu_stall_shd_mem[c_mem][resource_stall] = 0
gpgpu_stall_shd_mem[s_mem][bk_conf] = 0
gpgpu_stall_shd_mem[gl_mem][resource_stall] = 0
gpgpu_stall_shd_mem[gl_mem][coal_stall] = 0
gpgpu_stall_shd_mem[gl_mem][data_port_stall] = 0
gpu_reg_bank_conflict_stalls = 0
Warp Occupancy Distribution:
Stall:7	W0_Idle:1686	W0_Scoreboard:0	W1:0	W2:0	W3:0	W4:4	W5:0	W6:0	W7:0	W8:0	W9:0	W10:0	W11:0	W12:0	W13:0	W14:0	W15:0	W16:0	W17:0	W18:0	W19:0	W20:0	W21:0	W22:0	W23:0	W24:0	W25:0	W26:0	W27:0	W28:1	W29:0	W30:0	W31:0	W32:40
single_issue_nums: WS0:20	WS1:20	
dual_issue_nums: WS0:2	WS1:2	
traffic_breakdown_coretomem[GLOBAL_ACC_R] = 64 {8:8,}
traffic_breakdown_coretomem[GLOBAL_ACC_W] = 448 {40:1,136:3,}
traffic_breakdown_coretomem[INST_ACC_R] = 16 {8:2,}
traffic_breakdown_memtocore[GLOBAL_ACC_R] = 896 {40:2,136:6,}
traffic_breakdown_memtocore[GLOBAL_ACC_W] = 32 {8:4,}
traffic_breakdown_memtocore[INST_ACC_R] = 272 {136:2,}
maxmflatency = 280 
max_icnt2mem_latency = 23 
maxmrqlatency = 10 
max_icnt2sh_latency = 16 
mrq_lat_table:14 	0 	1 	3 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
dq_lat_table:0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
mf_lat_table:0 	0 	0 	0 	0 	0 	0 	0 	12 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
icnt2mem_lat_table:0 	0 	0 	10 	4 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
icnt2sh_lat_table:0 	0 	0 	11 	1 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
mf_lat_pw_table:0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
maximum concurrent accesses to same row:
dram[0]:         1         1         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[1]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[2]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[3]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[4]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[5]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[6]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[7]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
maximum service time to same row:
dram[0]:       302       504         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[1]:       506       508         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[2]:       830       810         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[3]:       817       819         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[4]:       837       816         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[5]:       825       828         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[6]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[7]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
average row accesses per activate:
dram[0]:  1.000000  1.000000      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan 
dram[1]:  1.000000  1.000000      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan 
dram[2]:  1.000000  1.000000      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan 
dram[3]:  1.000000  1.000000      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan 
dram[4]:  2.000000  2.000000      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan 
dram[5]:  2.000000  2.000000      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan 
dram[6]:      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan 
dram[7]:      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan 
average row locality = 18/14 = 1.285714
number of total memory accesses made:
dram[0]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[1]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[2]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[3]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[4]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[5]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[6]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[7]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
total accesses: 0
min_bank_accesses = 0!
min_chip_accesses = 0!
number of total read accesses:
dram[0]:         8         8         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[1]:         4         4         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[2]:         4         4         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[3]:         4         4         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[4]:         4         4         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[5]:         4         4         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[6]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[7]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
total dram reads = 56
min_bank_accesses = 0!
min_chip_accesses = 0!
number of total write accesses:
dram[0]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[1]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[2]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[3]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[4]:         4         4         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[5]:         4         1         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[6]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[7]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
total dram writes = 13
min_bank_accesses = 0!
min_chip_accesses = 0!
average mf latency per bank:
dram[0]:         34        33    none      none      none      none      none      none      none      none      none      none      none      none      none      none  
dram[1]:         67        68    none      none      none      none      none      none      none      none      none      none      none      none      none      none  
dram[2]:         70        66    none      none      none      none      none      none      none      none      none      none      none      none      none      none  
dram[3]:         68        69    none      none      none      none      none      none      none      none      none      none      none      none      none      none  
dram[4]:         33        32    none      none      none      none      none      none      none      none      none      none      none      none      none      none  
dram[5]:         33        52    none      none      none      none      none      none      none      none      none      none      none      none      none      none  
dram[6]:     none      none      none      none      none      none      none      none      none      none      none      none      none      none      none      none  
dram[7]:     none      none      none      none      none      none      none      none      none      none      none      none      none      none      none      none  
maximum mf latency per bank:
dram[0]:        273       265         0         0         0         0         0         0         0         0         0         0         0         0         0         0
dram[1]:        270       272         0         0         0         0         0         0         0         0         0         0         0         0         0         0
dram[2]:        280       266         0         0         0         0         0         0         0         0         0         0         0         0         0         0
dram[3]:        274       276         0         0         0         0         0         0         0         0         0         0         0         0         0         0
dram[4]:        269       257         0         0         0         0         0         0         0         0         0         0         0         0         0         0
dram[5]:        264       264         0         0         0         0         0         0         0         0         0         0         0         0         0         0
dram[6]:          0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0
dram[7]:          0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0

Number of Memory Banks Accessed per Memory Operation per Warp (from 0):
0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
Average # of Memory Banks Accessed per Memory Operation per Warp=-nan

position of mrq chosen
0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	

average position of mrq chosen = -nan
Memory Partition 0: 
Cache L2_bank_000:
MSHR contents

Cache L2_bank_001:
MSHR contents

In Dram Latency Queue (total = 0): 
DRAM[0]: 16 bks, busW=4 BL=8 CL=12, tRRD=6 tCCD=2, tRCD=12 tRAS=28 tRP=12 tRC=40
n_cmd=1349 n_nop=1327 n_act=4 n_pre=2 n_ref_event=0 n_req=4 n_rd=16 n_rd_L2_A=0 n_write=0 n_wr_bk=0 bw_util=0.02372
n_activity=140 dram_eff=0.2286
bk0: 8a 1301i bk1: 8a 1301i bk2: 0a 1346i bk3: 0a 1349i bk4: 0a 1349i bk5: 0a 1349i bk6: 0a 1349i bk7: 0a 1349i bk8: 0a 1349i bk9: 0a 1349i bk10: 0a 1349i bk11: 0a 1349i bk12: 0a 1349i bk13: 0a 1349i bk14: 0a 1349i bk15: 0a 1350i 

------------------------------------------------------------------------

Row_Buffer_Locality = 0.000000
Row_Buffer_Locality_read = 0.000000
Row_Buffer_Locality_write = -nan
Bank_Level_Parallism = 1.282051
Bank_Level_Parallism_Col = 1.142857
Bank_Level_Parallism_Ready = 1.000000
write_to_read_ratio_blp_rw_average = 0.000000
GrpLevelPara = 1.000000 

BW Util details:
bwutil = 0.023721 
total_CMD = 1349 
util_bw = 32 
Wasted_Col = 37 
Wasted_Row = 12 
Idle = 1268 

BW Util Bottlenecks: 
RCDc_limit = 40 
RCDWRc_limit = 0 
WTRc_limit = 0 
RTWc_limit = 0 
CCDLc_limit = 0 
rwq = 0 
CCDLc_limit_alone = 0 
WTRc_limit_alone = 0 
RTWc_limit_alone = 0 

Commands details: 
total_CMD = 1349 
n_nop = 1327 
Read = 16 
Write = 0 
L2_Alloc = 0 
L2_WB = 0 
n_act = 4 
n_pre = 2 
n_ref = 0 
n_req = 4 
total_req = 16 

Dual Bus Interface Util: 
issued_total_row = 6 
issued_total_col = 16 
Row_Bus_Util =  0.004448 
CoL_Bus_Util = 0.011861 
Either_Row_CoL_Bus_Util = 0.016308 
Issued_on_Two_Bus_Simul_Util = 0.000000 
issued_two_Eff = 0.000000 
queue_avg = 0.000000 


dram_util_bins: 0 0 0 0 0 0 0 0 0 0
dram_eff_bins: 0 0 0 0 0 0 0 0 0 0
mrqq: max=0 avg=0
Memory Partition 1: 
Cache L2_bank_002:
MSHR contents

Cache L2_bank_003:
MSHR contents

In Dram Latency Queue (total = 0): 
DRAM[1]: 16 bks, busW=4 BL=8 CL=12, tRRD=6 tCCD=2, tRCD=12 tRAS=28 tRP=12 tRC=40
n_cmd=1349 n_nop=1339 n_act=2 n_pre=0 n_ref_event=0 n_req=2 n_rd=8 n_rd_L2_A=0 n_write=0 n_wr_bk=0 bw_util=0.01186
n_activity=46 dram_eff=0.3478
bk0: 4a 1331i bk1: 4a 1326i bk2: 0a 1349i bk3: 0a 1349i bk4: 0a 1349i bk5: 0a 1349i bk6: 0a 1349i bk7: 0a 1349i bk8: 0a 1349i bk9: 0a 1349i bk10: 0a 1349i bk11: 0a 1349i bk12: 0a 1349i bk13: 0a 1349i bk14: 0a 1349i bk15: 0a 1349i 

------------------------------------------------------------------------

Row_Buffer_Locality = 0.000000
Row_Buffer_Locality_read = 0.000000
Row_Buffer_Locality_write = -nan
Bank_Level_Parallism = 1.592593
Bank_Level_Parallism_Col = 1.461538
Bank_Level_Parallism_Ready = 1.125000
write_to_read_ratio_blp_rw_average = 0.000000
GrpLevelPara = 1.000000 

BW Util details:
bwutil = 0.011861 
total_CMD = 1349 
util_bw = 16 
Wasted_Col = 12 
Wasted_Row = 0 
Idle = 1321 

BW Util Bottlenecks: 
RCDc_limit = 18 
RCDWRc_limit = 0 
WTRc_limit = 0 
RTWc_limit = 0 
CCDLc_limit = 0 
rwq = 0 
CCDLc_limit_alone = 0 
WTRc_limit_alone = 0 
RTWc_limit_alone = 0 

Commands details: 
total_CMD = 1349 
n_nop = 1339 
Read = 8 
Write = 0 
L2_Alloc = 0 
L2_WB = 0 
n_act = 2 
n_pre = 0 
n_ref = 0 
n_req = 2 
total_req = 8 

Dual Bus Interface Util: 
issued_total_row = 2 
issued_total_col = 8 
Row_Bus_Util =  0.001483 
CoL_Bus_Util = 0.005930 
Either_Row_CoL_Bus_Util = 0.007413 
Issued_on_Two_Bus_Simul_Util = 0.000000 
issued_two_Eff = 0.000000 
queue_avg = 0.000000 


dram_util_bins: 0 0 0 0 0 0 0 0 0 0
dram_eff_bins: 0 0 0 0 0 0 0 0 0 0
mrqq: max=0 avg=0
Memory Partition 2: 
Cache L2_bank_004:
MSHR contents

Cache L2_bank_005:
MSHR contents

In Dram Latency Queue (total = 0): 
DRAM[2]: 16 bks, busW=4 BL=8 CL=12, tRRD=6 tCCD=2, tRCD=12 tRAS=28 tRP=12 tRC=40
n_cmd=1349 n_nop=1339 n_act=2 n_pre=0 n_ref_event=0 n_req=2 n_rd=8 n_rd_L2_A=0 n_write=0 n_wr_bk=0 bw_util=0.01186
n_activity=72 dram_eff=0.2222
bk0: 4a 1331i bk1: 4a 1330i bk2: 0a 1347i bk3: 0a 1349i bk4: 0a 1349i bk5: 0a 1349i bk6: 0a 1349i bk7: 0a 1349i bk8: 0a 1349i bk9: 0a 1349i bk10: 0a 1349i bk11: 0a 1349i bk12: 0a 1349i bk13: 0a 1349i bk14: 0a 1349i bk15: 0a 1350i 

------------------------------------------------------------------------

Row_Buffer_Locality = 0.000000
Row_Buffer_Locality_read = 0.000000
Row_Buffer_Locality_write = -nan
Bank_Level_Parallism = 1.000000
Bank_Level_Parallism_Col = 1.000000
Bank_Level_Parallism_Ready = 1.000000
write_to_read_ratio_blp_rw_average = 0.000000
GrpLevelPara = 1.000000 

BW Util details:
bwutil = 0.011861 
total_CMD = 1349 
util_bw = 16 
Wasted_Col = 24 
Wasted_Row = 0 
Idle = 1309 

BW Util Bottlenecks: 
RCDc_limit = 24 
RCDWRc_limit = 0 
WTRc_limit = 0 
RTWc_limit = 0 
CCDLc_limit = 0 
rwq = 0 
CCDLc_limit_alone = 0 
WTRc_limit_alone = 0 
RTWc_limit_alone = 0 

Commands details: 
total_CMD = 1349 
n_nop = 1339 
Read = 8 
Write = 0 
L2_Alloc = 0 
L2_WB = 0 
n_act = 2 
n_pre = 0 
n_ref = 0 
n_req = 2 
total_req = 8 

Dual Bus Interface Util: 
issued_total_row = 2 
issued_total_col = 8 
Row_Bus_Util =  0.001483 
CoL_Bus_Util = 0.005930 
Either_Row_CoL_Bus_Util = 0.007413 
Issued_on_Two_Bus_Simul_Util = 0.000000 
issued_two_Eff = 0.000000 
queue_avg = 0.000000 


dram_util_bins: 0 0 0 0 0 0 0 0 0 0
dram_eff_bins: 0 0 0 0 0 0 0 0 0 0
mrqq: max=0 avg=0
Memory Partition 3: 
Cache L2_bank_006:
MSHR contents

Cache L2_bank_007:
MSHR contents

In Dram Latency Queue (total = 0): 
DRAM[3]: 16 bks, busW=4 BL=8 CL=12, tRRD=6 tCCD=2, tRCD=12 tRAS=28 tRP=12 tRC=40
n_cmd=1349 n_nop=1339 n_act=2 n_pre=0 n_ref_event=0 n_req=2 n_rd=8 n_rd_L2_A=0 n_write=0 n_wr_bk=0 bw_util=0.01186
n_activity=46 dram_eff=0.3478
bk0: 4a 1331i bk1: 4a 1326i bk2: 0a 1349i bk3: 0a 1349i bk4: 0a 1349i bk5: 0a 1349i bk6: 0a 1349i bk7: 0a 1349i bk8: 0a 1349i bk9: 0a 1349i bk10: 0a 1349i bk11: 0a 1349i bk12: 0a 1349i bk13: 0a 1349i bk14: 0a 1349i bk15: 0a 1349i 

------------------------------------------------------------------------

Row_Buffer_Locality = 0.000000
Row_Buffer_Locality_read = 0.000000
Row_Buffer_Locality_write = -nan
Bank_Level_Parallism = 1.592593
Bank_Level_Parallism_Col = 1.461538
Bank_Level_Parallism_Ready = 1.125000
write_to_read_ratio_blp_rw_average = 0.000000
GrpLevelPara = 1.000000 

BW Util details:
bwutil = 0.011861 
total_CMD = 1349 
util_bw = 16 
Wasted_Col = 12 
Wasted_Row = 0 
Idle = 1321 

BW Util Bottlenecks: 
RCDc_limit = 18 
RCDWRc_limit = 0 
WTRc_limit = 0 
RTWc_limit = 0 
CCDLc_limit = 0 
rwq = 0 
CCDLc_limit_alone = 0 
WTRc_limit_alone = 0 
RTWc_limit_alone = 0 

Commands details: 
total_CMD = 1349 
n_nop = 1339 
Read = 8 
Write = 0 
L2_Alloc = 0 
L2_WB = 0 
n_act = 2 
n_pre = 0 
n_ref = 0 
n_req = 2 
total_req = 8 

Dual Bus Interface Util: 
issued_total_row = 2 
issued_total_col = 8 
Row_Bus_Util =  0.001483 
CoL_Bus_Util = 0.005930 
Either_Row_CoL_Bus_Util = 0.007413 
Issued_on_Two_Bus_Simul_Util = 0.000000 
issued_two_Eff = 0.000000 
queue_avg = 0.000000 


dram_util_bins: 0 0 0 0 0 0 0 0 0 0
dram_eff_bins: 0 0 0 0 0 0 0 0 0 0
mrqq: max=0 avg=0
Memory Partition 4: 
Cache L2_bank_008:
MSHR contents

Cache L2_bank_009:
MSHR contents

In Dram Latency Queue (total = 0): 
DRAM[4]: 16 bks, busW=4 BL=8 CL=12, tRRD=6 tCCD=2, tRCD=12 tRAS=28 tRP=12 tRC=40
n_cmd=1349 n_nop=1331 n_act=2 n_pre=0 n_ref_event=0 n_req=4 n_rd=0 n_rd_L2_A=8 n_write=8 n_wr_bk=0 bw_util=0.02372
n_activity=77 dram_eff=0.4156
bk0: 4a 1318i bk1: 4a 1319i bk2: 0a 1347i bk3: 0a 1348i bk4: 0a 1349i bk5: 0a 1349i bk6: 0a 1349i bk7: 0a 1349i bk8: 0a 1349i bk9: 0a 1349i bk10: 0a 1349i bk11: 0a 1349i bk12: 0a 1349i bk13: 0a 1349i bk14: 0a 1350i bk15: 0a 1350i 

------------------------------------------------------------------------

Row_Buffer_Locality = 0.500000
Row_Buffer_Locality_read = 1.000000
Row_Buffer_Locality_write = 0.000000
Bank_Level_Parallism = 1.000000
Bank_Level_Parallism_Col = 1.000000
Bank_Level_Parallism_Ready = 1.000000
write_to_read_ratio_blp_rw_average = 0.451613
GrpLevelPara = 1.000000 

BW Util details:
bwutil = 0.023721 
total_CMD = 1349 
util_bw = 32 
Wasted_Col = 34 
Wasted_Row = 0 
Idle = 1283 

BW Util Bottlenecks: 
RCDc_limit = 0 
RCDWRc_limit = 14 
WTRc_limit = 18 
RTWc_limit = 9 
CCDLc_limit = 0 
rwq = 0 
CCDLc_limit_alone = 0 
WTRc_limit_alone = 18 
RTWc_limit_alone = 9 

Commands details: 
total_CMD = 1349 
n_nop = 1331 
Read = 0 
Write = 8 
L2_Alloc = 8 
L2_WB = 0 
n_act = 2 
n_pre = 0 
n_ref = 0 
n_req = 4 
total_req = 16 

Dual Bus Interface Util: 
issued_total_row = 2 
issued_total_col = 16 
Row_Bus_Util =  0.001483 
CoL_Bus_Util = 0.011861 
Either_Row_CoL_Bus_Util = 0.013343 
Issued_on_Two_Bus_Simul_Util = 0.000000 
issued_two_Eff = 0.000000 
queue_avg = 0.020756 


dram_util_bins: 0 0 0 0 0 0 0 0 0 0
dram_eff_bins: 0 0 0 0 0 0 0 0 0 0
mrqq: max=1 avg=0.0207561
Memory Partition 5: 
Cache L2_bank_010:
MSHR contents

Cache L2_bank_011:
MSHR contents

In Dram Latency Queue (total = 0): 
DRAM[5]: 16 bks, busW=4 BL=8 CL=12, tRRD=6 tCCD=2, tRCD=12 tRAS=28 tRP=12 tRC=40
n_cmd=1349 n_nop=1334 n_act=2 n_pre=0 n_ref_event=0 n_req=4 n_rd=0 n_rd_L2_A=8 n_write=5 n_wr_bk=0 bw_util=0.01927
n_activity=52 dram_eff=0.5
bk0: 4a 1318i bk1: 4a 1315i bk2: 0a 1349i bk3: 0a 1349i bk4: 0a 1349i bk5: 0a 1349i bk6: 0a 1349i bk7: 0a 1349i bk8: 0a 1349i bk9: 0a 1349i bk10: 0a 1349i bk11: 0a 1349i bk12: 0a 1349i bk13: 0a 1349i bk14: 0a 1349i bk15: 0a 1349i 

------------------------------------------------------------------------

Row_Buffer_Locality = 0.500000
Row_Buffer_Locality_read = 1.000000
Row_Buffer_Locality_write = 0.000000
Bank_Level_Parallism = 1.682927
Bank_Level_Parallism_Col = 1.650000
Bank_Level_Parallism_Ready = 1.384615
write_to_read_ratio_blp_rw_average = 0.350000
GrpLevelPara = 1.000000 

BW Util details:
bwutil = 0.019274 
total_CMD = 1349 
util_bw = 26 
Wasted_Col = 16 
Wasted_Row = 0 
Idle = 1307 

BW Util Bottlenecks: 
RCDc_limit = 1 
RCDWRc_limit = 8 
WTRc_limit = 18 
RTWc_limit = 0 
CCDLc_limit = 0 
rwq = 0 
CCDLc_limit_alone = 0 
WTRc_limit_alone = 18 
RTWc_limit_alone = 0 

Commands details: 
total_CMD = 1349 
n_nop = 1334 
Read = 0 
Write = 5 
L2_Alloc = 8 
L2_WB = 0 
n_act = 2 
n_pre = 0 
n_ref = 0 
n_req = 4 
total_req = 13 

Dual Bus Interface Util: 
issued_total_row = 2 
issued_total_col = 13 
Row_Bus_Util =  0.001483 
CoL_Bus_Util = 0.009637 
Either_Row_CoL_Bus_Util = 0.011119 
Issued_on_Two_Bus_Simul_Util = 0.000000 
issued_two_Eff = 0.000000 
queue_avg = 0.015567 


dram_util_bins: 0 0 0 0 0 0 0 0 0 0
dram_eff_bins: 0 0 0 0 0 0 0 0 0 0
mrqq: max=2 avg=0.0155671
Memory Partition 6: 
Cache L2_bank_012:
MSHR contents

Cache L2_bank_013:
MSHR contents

In Dram Latency Queue (total = 0): 
DRAM[6]: 16 bks, busW=4 BL=8 CL=12, tRRD=6 tCCD=2, tRCD=12 tRAS=28 tRP=12 tRC=40
n_cmd=1349 n_nop=1349 n_act=0 n_pre=0 n_ref_event=0 n_req=0 n_rd=0 n_rd_L2_A=0 n_write=0 n_wr_bk=0 bw_util=0
n_activity=0 dram_eff=-nan
bk0: 0a 1349i bk1: 0a 1349i bk2: 0a 1349i bk3: 0a 1349i bk4: 0a 1349i bk5: 0a 1349i bk6: 0a 1349i bk7: 0a 1349i bk8: 0a 1349i bk9: 0a 1349i bk10: 0a 1349i bk11: 0a 1349i bk12: 0a 1349i bk13: 0a 1349i bk14: 0a 1349i bk15: 0a 1349i 

------------------------------------------------------------------------

Row_Buffer_Locality = -nan
Row_Buffer_Locality_read = -nan
Row_Buffer_Locality_write = -nan
Bank_Level_Parallism = -nan
Bank_Level_Parallism_Col = -nan
Bank_Level_Parallism_Ready = -nan
write_to_read_ratio_blp_rw_average = -nan
GrpLevelPara = -nan 

BW Util details:
bwutil = 0.000000 
total_CMD = 1349 
util_bw = 0 
Wasted_Col = 0 
Wasted_Row = 0 
Idle = 1349 

BW Util Bottlenecks: 
RCDc_limit = 0 
RCDWRc_limit = 0 
WTRc_limit = 0 
RTWc_limit = 0 
CCDLc_limit = 0 
rwq = 0 
CCDLc_limit_alone = 0 
WTRc_limit_alone = 0 
RTWc_limit_alone = 0 

Commands details: 
total_CMD = 1349 
n_nop = 1349 
Read = 0 
Write = 0 
L2_Alloc = 0 
L2_WB = 0 
n_act = 0 
n_pre = 0 
n_ref = 0 
n_req = 0 
total_req = 0 

Dual Bus Interface Util: 
issued_total_row = 0 
issued_total_col = 0 
Row_Bus_Util =  0.000000 
CoL_Bus_Util = 0.000000 
Either_Row_CoL_Bus_Util = 0.000000 
Issued_on_Two_Bus_Simul_Util = 0.000000 
issued_two_Eff = -nan 
queue_avg = 0.000000 


dram_util_bins: 0 0 0 0 0 0 0 0 0 0
dram_eff_bins: 0 0 0 0 0 0 0 0 0 0
mrqq: max=0 avg=0
Memory Partition 7: 
Cache L2_bank_014:
MSHR contents

Cache L2_bank_015:
MSHR contents

In Dram Latency Queue (total = 0): 
DRAM[7]: 16 bks, busW=4 BL=8 CL=12, tRRD=6 tCCD=2, tRCD=12 tRAS=28 tRP=12 tRC=40
n_cmd=1349 n_nop=1349 n_act=0 n_pre=0 n_ref_event=0 n_req=0 n_rd=0 n_rd_L2_A=0 n_write=0 n_wr_bk=0 bw_util=0
n_activity=0 dram_eff=-nan
bk0: 0a 1349i bk1: 0a 1349i bk2: 0a 1349i bk3: 0a 1349i bk4: 0a 1349i bk5: 0a 1349i bk6: 0a 1349i bk7: 0a 1349i bk8: 0a 1349i bk9: 0a 1349i bk10: 0a 1349i bk11: 0a 1349i bk12: 0a 1349i bk13: 0a 1349i bk14: 0a 1349i bk15: 0a 1349i 

------------------------------------------------------------------------

Row_Buffer_Locality = -nan
Row_Buffer_Locality_read = -nan
Row_Buffer_Locality_write = -nan
Bank_Level_Parallism = -nan
Bank_Level_Parallism_Col = -nan
Bank_Level_Parallism_Ready = -nan
write_to_read_ratio_blp_rw_average = -nan
GrpLevelPara = -nan 

BW Util details:
bwutil = 0.000000 
total_CMD = 1349 
util_bw = 0 
Wasted_Col = 0 
Wasted_Row = 0 
Idle = 1349 

BW Util Bottlenecks: 
RCDc_limit = 0 
RCDWRc_limit = 0 
WTRc_limit = 0 
RTWc_limit = 0 
CCDLc_limit = 0 
rwq = 0 
CCDLc_limit_alone = 0 
WTRc_limit_alone = 0 
RTWc_limit_alone = 0 

Commands details: 
total_CMD = 1349 
n_nop = 1349 
Read = 0 
Write = 0 
L2_Alloc = 0 
L2_WB = 0 
n_act = 0 
n_pre = 0 
n_ref = 0 
n_req = 0 
total_req = 0 

Dual Bus Interface Util: 
issued_total_row = 0 
issued_total_col = 0 
Row_Bus_Util =  0.000000 
CoL_Bus_Util = 0.000000 
Either_Row_CoL_Bus_Util = 0.000000 
Issued_on_Two_Bus_Simul_Util = 0.000000 
issued_two_Eff = -nan 
queue_avg = 0.000000 


dram_util_bins: 0 0 0 0 0 0 0 0 0 0
dram_eff_bins: 0 0 0 0 0 0 0 0 0 0
mrqq: max=0 avg=0

========= L2 cache stats =========
L2_cache_bank[0]: Access = 2, Miss = 2, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[1]: Access = 2, Miss = 2, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[2]: Access = 1, Miss = 1, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[3]: Access = 1, Miss = 1, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[4]: Access = 1, Miss = 1, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[5]: Access = 1, Miss = 1, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[6]: Access = 1, Miss = 1, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[7]: Access = 1, Miss = 1, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[8]: Access = 1, Miss = 1, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[9]: Access = 1, Miss = 1, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[10]: Access = 1, Miss = 1, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[11]: Access = 1, Miss = 1, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[12]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[13]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[14]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[15]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
L2_total_cache_accesses = 14
L2_total_cache_misses = 14
L2_total_cache_miss_rate = 1.0000
L2_total_cache_pending_hits = 0
L2_total_cache_reservation_fails = 0
L2_total_cache_breakdown:
	L2_cache_stats_breakdown[GLOBAL_ACC_R][HIT] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_R][MISS] = 8
	L2_cache_stats_breakdown[GLOBAL_ACC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_R][MSHR_HIT] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_R][HIT] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_R][MISS] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_R][MSHR_HIT] = 0
	L2_cache_stats_breakdown[CONST_ACC_R][HIT] = 0
	L2_cache_stats_breakdown[CONST_ACC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[CONST_ACC_R][MISS] = 0
	L2_cache_stats_breakdown[CONST_ACC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[CONST_ACC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[CONST_ACC_R][MSHR_HIT] = 0
	L2_cache_stats_breakdown[TEXTURE_ACC_R][HIT] = 0
	L2_cache_stats_breakdown[TEXTURE_ACC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[TEXTURE_ACC_R][MISS] = 0
	L2_cache_stats_breakdown[TEXTURE_ACC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[TEXTURE_ACC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[TEXTURE_ACC_R][MSHR_HIT] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_W][HIT] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_W][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_W][MISS] = 4
	L2_cache_stats_breakdown[GLOBAL_ACC_W][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_W][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_W][MSHR_HIT] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_W][HIT] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_W][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_W][MISS] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_W][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_W][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_W][MSHR_HIT] = 0
	L2_cache_stats_breakdown[L1_WRBK_ACC][HIT] = 0
	L2_cache_stats_breakdown[L1_WRBK_ACC][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[L1_WRBK_ACC][MISS] = 0
	L2_cache_stats_breakdown[L1_WRBK_ACC][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[L1_WRBK_ACC][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[L1_WRBK_ACC][MSHR_HIT] = 0
	L2_cache_stats_breakdown[L2_WRBK_ACC][HIT] = 0
	L2_cache_stats_breakdown[L2_WRBK_ACC][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[L2_WRBK_ACC][MISS] = 0
	L2_cache_stats_breakdown[L2_WRBK_ACC][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[L2_WRBK_ACC][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[L2_WRBK_ACC][MSHR_HIT] = 0
	L2_cache_stats_breakdown[INST_ACC_R][HIT] = 0
	L2_cache_stats_breakdown[INST_ACC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[INST_ACC_R][MISS] = 2
	L2_cache_stats_breakdown[INST_ACC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[INST_ACC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[INST_ACC_R][MSHR_HIT] = 0
	L2_cache_stats_breakdown[L1_WR_ALLOC_R][HIT] = 0
	L2_cache_stats_breakdown[L1_WR_ALLOC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[L1_WR_ALLOC_R][MISS] = 0
	L2_cache_stats_breakdown[L1_WR_ALLOC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[L1_WR_ALLOC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[L1_WR_ALLOC_R][MSHR_HIT] = 0
	L2_cache_stats_breakdown[L2_WR_ALLOC_R][HIT] = 0
	L2_cache_stats_breakdown[L2_WR_ALLOC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[L2_WR_ALLOC_R][MISS] = 0
	L2_cache_stats_breakdown[L2_WR_ALLOC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[L2_WR_ALLOC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[L2_WR_ALLOC_R][MSHR_HIT] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_R][TOTAL_ACCESS] = 8
	L2_cache_stats_breakdown[GLOBAL_ACC_W][TOTAL_ACCESS] = 4
	L2_cache_stats_breakdown[INST_ACC_R][TOTAL_ACCESS] = 2
L2_total_cache_reservation_fail_breakdown:
L2_cache_data_port_util = 0.000
L2_cache_fill_port_util = 0.004

icnt_total_pkts_mem_to_simt=48
icnt_total_pkts_simt_to_mem=27
LD_mem_lat_dist  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
ST_mem_lat_dist  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
----------------------------Interconnect-DETAILS--------------------------------
Class 0:
Packet latency average = 10.5
	minimum = 6
	maximum = 21
Network latency average = 9.35714
	minimum = 6
	maximum = 16
Slowest packet = 8
Flit latency average = 7.81333
	minimum = 6
	maximum = 12
Slowest flit = 48
Fragmentation average = 0
	minimum = 0
	maximum = 0
Injected packet rate average = 0.000645161
	minimum = 0 (at node 1)
	maximum = 0.016129 (at node 0)
Accepted packet rate average = 0.000645161
	minimum = 0 (at node 1)
	maximum = 0.016129 (at node 0)
Injected flit rate average = 0.00172811
	minimum = 0 (at node 1)
	maximum = 0.031106 (at node 0)
Accepted flit rate average= 0.00172811
	minimum = 0 (at node 1)
	maximum = 0.0552995 (at node 0)
Injected packet length average = 2.67857
Accepted packet length average = 2.67857
Total in-flight flits = 0 (0 measured)
====== Overall Traffic Statistics ======
====== Traffic class 0 ======
Packet latency average = 10.5 (1 samples)
	minimum = 6 (1 samples)
	maximum = 21 (1 samples)
Network latency average = 9.35714 (1 samples)
	minimum = 6 (1 samples)
	maximum = 16 (1 samples)
Flit latency average = 7.81333 (1 samples)
	minimum = 6 (1 samples)
	maximum = 12 (1 samples)
Fragmentation average = 0 (1 samples)
	minimum = 0 (1 samples)
	maximum = 0 (1 samples)
Injected packet rate average = 0.000645161 (1 samples)
	minimum = 0 (1 samples)
	maximum = 0.016129 (1 samples)
Accepted packet rate average = 0.000645161 (1 samples)
	minimum = 0 (1 samples)
	maximum = 0.016129 (1 samples)
Injected flit rate average = 0.00172811 (1 samples)
	minimum = 0 (1 samples)
	maximum = 0.031106 (1 samples)
Accepted flit rate average = 0.00172811 (1 samples)
	minimum = 0 (1 samples)
	maximum = 0.0552995 (1 samples)
Injected packet size average = 2.67857 (1 samples)
Accepted packet size average = 2.67857 (1 samples)
Hops average = 1 (1 samples)
----------------------------END-of-Interconnect-DETAILS-------------------------


gpgpu_simulation_time = 0 days, 0 hrs, 0 min, 1 sec (1 sec)
gpgpu_simulation_rate = 1324 (inst/sec)
gpgpu_simulation_rate = 868 (cycle/sec)
gpgpu_silicon_slowdown = 1851382x
GPGPU-Sim: *** simulation thread exiting ***
GPGPU-Sim: *** exit detected ***
